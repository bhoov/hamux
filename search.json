[
  {
    "objectID": "lagrangians.html",
    "href": "lagrangians.html",
    "title": "Lagrangians",
    "section": "",
    "text": "TL;DR\n\n\n\n\nAll dynamics of Associative Memory are constrained by a Lagrangian\nThe Lagrangian is a convex, scalar-valued function\nA neuron‚Äôs activations are the derivative of that neuron‚Äôs Lagrangian. These activations are often non-linear functions of the dynamic state, and look like the activation functions that we see in many modern Neural Networks (e.g., sigmoid, tanh, relu, softmax, LayerNorm, etc.)\nLagrangian functions are fundamental to the energy of üåÄneuron layers. These convex functions can be seen as the integral of common activation functions (e.g., relus and softmaxes). All Lagrangians are functions of the form:\n\\[\\mathcal{L}(\\mathbf{x};\\ldots) \\mapsto \\mathbb{R}\\]\nwhere \\(\\mathbf{x} \\in \\mathbb{R}^{D_1 \\times \\ldots \\times D_n}\\) can be a tensor of arbitrary shape and \\(\\mathcal{L}\\) can be optionally parameterized (e.g., the LayerNorm‚Äôs learnable bias and scale). Lagrangians must be convex and differentiable.\nWe want to rely on JAX‚Äôs autograd to automatically differentiate our Lagrangians into activation functions. For certain Lagrangians, the naively autodiff-ed function of the defined Lagrangian is numerically unstable (e.g., lagr_sigmoid(x) and lagr_tanh(x)). In these cases, we follow JAX‚Äôs documentation guidelines to define custom_jvps to fix this behavior.",
    "crumbs": [
      "Lagrangians"
    ]
  },
  {
    "objectID": "lagrangians.html#elementwise-lagrangians",
    "href": "lagrangians.html#elementwise-lagrangians",
    "title": "Lagrangians",
    "section": "Elementwise Lagrangians",
    "text": "Elementwise Lagrangians\nThough we define Lagrangians for an entire tensor, these special ‚Äúelementwise Lagrangians‚Äù take a special form: they are simply the sum of the convex, differentiable function applied elementwise to the underlying tensor. This makes it easy to plot and visualize them.\nLet‚Äôs look at what some of these Lagrangians look like in practice.\n\n\nlagr_identity\n\n lagr_identity (x:jax.Array)\n\nThe Lagrangian whose activation function is simply the identity.\n\n\n\n\nType\nDetails\n\n\n\n\nx\nArray\nInput tensor\n\n\nReturns\nFloat\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{identity}(\\mathbf{x}) &= \\frac{1}{2} \\sum_i x_i^2 \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{identity}(\\mathbf{x}) &= x_i\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nlagr_repu\n\n lagr_repu (x:jax.Array, n:float)\n\nRectified Power Unit of degree n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nArray\nInput tensor\n\n\nn\nfloat\nDegree of the polynomial in the power unit\n\n\nReturns\nFloat\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{RePU}(\\mathbf{x}; n) &= \\frac{1}{n} \\sum_i \\max(x_i, 0)^n \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{RePU}(\\mathbf{x}; n) &= \\max(x_i, 0)^{n-1}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nlagr_relu\n\n lagr_relu (x:jax.Array)\n\nRectified Linear Unit. Same as lagr_repu of degree 2\n\n\n\n\nType\nDetails\n\n\n\n\nx\nArray\nInput tensor\n\n\nReturns\nFloat\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{relu}(\\mathbf{x}) &= \\frac{1}{2} \\sum_i \\max(x_i, 0)^2 \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{relu}(\\mathbf{x}) &= \\max(x_i, 0)\n\\end{align*}\n\\]\n\n\n\nlagr_exp\n\n lagr_exp (x:jax.Array, beta:float=1.0)\n\nExponential activation function, as in Demicirgil et al.. Operates elementwise\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\nInput tensor\n\n\nbeta\nfloat\n1.0\nInverse temperature\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{exp}(\\mathbf{x}; \\beta) &= \\frac{1}{\\beta} \\sum_i e^{\\beta x_i} \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{exp}(\\mathbf{x}; \\beta) &= e^{\\beta x_i}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nlagr_rexp\n\n lagr_rexp (x:jax.Array, beta:float=1.0)\n\nLagrangian of the Rectified exponential activation function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\n\n\n\nbeta\nfloat\n1.0\nInverse temperature\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\n\n\nlagr_tanh\n\n lagr_tanh (x:jax.Array, beta:float=1.0)\n\nLagrangian of the tanh activation function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\nInput tensor\n\n\nbeta\nfloat\n1.0\nInverse temperature\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{tanh}(\\mathbf{x}; \\beta) &= \\frac{1}{\\beta} \\sum_i \\log(\\cosh(\\beta x_i)) \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{tanh}(\\mathbf{x}; \\beta) &= \\tanh(\\beta x_i)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nlagr_sigmoid\n\n lagr_sigmoid (x:jax.Array, beta:float=1.0)\n\nThe lagrangian of the sigmoid activation function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\nInput tensor\n\n\nbeta\nfloat\n1.0\nInverse temperature\n\n\nReturns\nFloat\n\nOutput scalar",
    "crumbs": [
      "Lagrangians"
    ]
  },
  {
    "objectID": "lagrangians.html#lagrangians-with-competing-units",
    "href": "lagrangians.html#lagrangians-with-competing-units",
    "title": "Lagrangians",
    "section": "Lagrangians with competing units",
    "text": "Lagrangians with competing units\nWe can define Lagrangians where activations are normalized in some way (i.e., where the derivative of the Lagrangian introduces some normalization factor). There are many forms of activation functions in modern Deep Learning with this structure; e.g., softmaxes, layernorms, etc. normalize their input by some value. There is a nice interpretation of these kinds of activation functions as competing hidden units.\n\n\nlagr_softmax\n\n lagr_softmax (x:jax.Array, beta:float=1.0, axis:int=-1)\n\nThe lagrangian of the softmax ‚Äì the logsumexp\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\nInput tensor\n\n\nbeta\nfloat\n1.0\nInverse temperature\n\n\naxis\nint\n-1\nDimension over which to apply logsumexp\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{softmax}(\\mathbf{x}; \\beta) &= \\frac{1}{\\beta} \\log \\sum_i e^{\\beta x_i} \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{softmax}(\\mathbf{x}; \\beta) &= \\frac{e^{\\beta x_i}}{\\sum_j e^{\\beta x_j}}\n\\end{align*}\n\\]\nWe plot its activations (the softmax) for a vector of length 10 below.\n\n\n\n\n\n\n\n\n\n\n\n\nlagr_layernorm\n\n lagr_layernorm (x:jax.Array, gamma:float=1.0,\n                 delta:Union[float,jax.Array]=0.0, axis:int=-1,\n                 eps:float=1e-05)\n\n*Lagrangian of the layer norm activation function.\ngamma must be a float, not a vector.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\nInput tensor\n\n\ngamma\nfloat\n1.0\nScale the stdev\n\n\ndelta\nUnion\n0.0\nShift the mean\n\n\naxis\nint\n-1\nWhich axis to normalize\n\n\neps\nfloat\n1e-05\nPrevent division by 0\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{layernorm}(\\mathbf{x}; \\gamma, \\delta) &= D \\gamma \\sqrt{\\text{Var}(\\mathbf{x}) + \\epsilon} + \\sum_i \\delta_i x_i \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{layernorm}(\\mathbf{x}; \\gamma, \\delta) &= \\gamma \\frac{x_i - \\text{Mean}(\\mathbf{x})}{\\sqrt{\\text{Var}(\\mathbf{x}) + \\epsilon}} + \\delta_i\n\\end{align*}\n\\]\n\n\n\nlagr_spherical_norm\n\n lagr_spherical_norm (x:jax.Array, gamma:float=1.0,\n                      delta:Union[float,jax.Array]=0.0, axis:int=-1,\n                      eps:float=1e-05)\n\nLagrangian of the spherical norm (L2 norm) activation function\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nArray\n\ninput tensor\n\n\ngamma\nfloat\n1.0\nScale the stdev\n\n\ndelta\nUnion\n0.0\nShift the mean\n\n\naxis\nint\n-1\nWhich axis to normalize\n\n\neps\nfloat\n1e-05\nPrevent division by 0\n\n\nReturns\nFloat\n\nOutput scalar\n\n\n\n\\[\n\\begin{align*}\n\\mathcal{L}_\\text{L2norm}(\\mathbf{x}; \\gamma, \\delta) &= \\gamma \\sqrt{\\sum_i x_i^2 + \\epsilon} + \\sum_i \\delta_i x_i \\\\\n\\partial_{x_i} \\mathcal{L}_\\text{L2norm}(\\mathbf{x}; \\gamma, \\delta) &= \\gamma \\frac{x_i}{\\sqrt{\\sum_j x_j^2 + \\epsilon}} + \\delta_i\n\\end{align*}\n\\]",
    "crumbs": [
      "Lagrangians"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Building HAMUX",
    "section": "",
    "text": "We develop a modular energy perspective of Associative Memories where the energy of any model from this family can be decomposed into standardized components: neuron layers that encode dynamic variables, and hypersynapses that encode their interactions.\nThe total energy of the system is the sum of the individual component energies subtracted by the energies of all the layers and all the interactions between those layers.\nIn computer science terms, neurons and synapses form a hypergraph, where each neuron layer is a node and each hypersynapse is a hyperedge.\nThis framework of energy-based building blocks for memory not only clarifies how existing methods for building Associative Memories relate to each other (e.g., the Classical Hopfield Network [@hopfield1982neural, @hopfield1984Neurons], Dense Associative Memory [@krotov2016dense]), but it also provides a systematic language for designing new architectures (e.g., Energy Transformers [@hoover2024energy], Neuron-Astrocyte Networks [@kozachkov2023neuron]).\nWe begin by introducing the building blocks of Associative Memory: neurons and hypersynapses.",
    "crumbs": [
      "Building HAMUX"
    ]
  },
  {
    "objectID": "core.html#sec-neuron-implementation",
    "href": "core.html#sec-neuron-implementation",
    "title": "Building HAMUX",
    "section": "Neurons as Code",
    "text": "Neurons as Code\nThe methods to implement neurons are remarkably simple:\n\n\nNeuronLayer\n\n NeuronLayer (lagrangian:Callable, shape:Tuple[int])\n\nNeuron layers represent dynamic variables that evolve during inference (i.e., memory retrieval/error correction)\n\n\nExported source\nclass NeuronLayer(eqx.Module):\n    \"\"\"Neuron layers represent dynamic variables that evolve during inference (i.e., memory retrieval/error correction)\"\"\"\n    lagrangian: Callable # The scalar-valued Lagrangian function:  x |-&gt; R\n    shape: Tuple[int] # The shape of the neuron layer\n\n\nRemember that, at its core, a NeuronLayer object is nothing more than a Lagrangian (see example Lagrangians for examples) function on top of (shaped) data. All the other methods of the NeuronLayer class just provide conveniences on top of this core functionality.\n\n\n\nNeuronLayer.activations\n\n NeuronLayer.activations (x)\n\nUse autograd to compute the activations of the neuron layer from the Lagrangian\n\n\nExported source\n@patch\ndef activations(self: NeuronLayer, x): \n    \"\"\"Use autograd to compute the activations of the neuron layer from the Lagrangian\"\"\"\n    return jax.grad(self.lagrangian)(x)\n\n\nThe NeuronLayer.activations is the gradient of the Lagrangian with respect to the states. This is easily computed via jax autograd.\n\n\n\n\n\n\nTest the activations\n\n\n\n\n\nFor example, we can test the activations of a few different Lagrangians.\n\nfrom hamux.lagrangians import lagr_relu, lagr_softmax, lagr_sigmoid, lagr_identity\nfrom pprint import pp\n\n\nD = 10\n\n# Identity activation\nnn = NeuronLayer(lagrangian=lambda x: jnp.sum(0.5 * x**2), shape=(D,)) # Identity activation\nxtest = jr.normal(jr.key(0), (D,))\nassert jnp.allclose(nn.activations(xtest), xtest)\n\n# ReLU activation\nnn = NeuronLayer(lagrangian=lagr_relu, shape=(D,))\nxtest = jr.normal(jr.key(1), (D,))\nassert jnp.allclose(nn.activations(xtest), jnp.maximum(0, xtest))\n\n# Softmax activation\nnn = NeuronLayer(lagrangian=lagr_softmax, shape=(D,))\nxtest = jr.normal(jr.key(2), (D,))\nassert jnp.allclose(nn.activations(xtest), jax.nn.softmax(xtest))\n\n\n\n\n\n\n\nNeuronLayer.init\n\n NeuronLayer.init (bs:Optional[int]=None)\n\nReturn an empty initial neuron state\n\n\nExported source\n@patch\ndef init(self: NeuronLayer, bs: Optional[int] = None):\n    \"\"\"Return an empty initial neuron state\"\"\"\n    if bs is None or bs == 0: return jnp.zeros(self.shape) # No batch dimension\n    return jnp.zeros((bs, *self.shape))\n\n\nThe NeuronLayer.init method is a convenience method that initializes an empty collection of neuron layer states. We generally want to populate this state with values from some piece of data.\nIt‚Äôs used like:\n\nD = 784 # e.g., rasterized MNIST image size\nnn = NeuronLayer(lagrangian=lambda x: 0.5 * x**2, shape=(D,))\nx_unbatched = nn.init()\nprint(\"Unbatched shape:\", x_unbatched.shape)\nx_batched = nn.init(bs=2)\nprint(\"Batched shape:\", x_batched.shape)\n\nUnbatched shape: (784,)\nBatched shape: (2, 784)",
    "crumbs": [
      "Building HAMUX"
    ]
  },
  {
    "objectID": "core.html#legendre-transform-and-neuron-energy",
    "href": "core.html#legendre-transform-and-neuron-energy",
    "title": "Building HAMUX",
    "section": "Legendre transform and Neuron Energy",
    "text": "Legendre transform and Neuron Energy\nThe energy is the Legendre transform of the Lagrangian. Consider some scalar-valued function \\(F: \\mathcal{X} \\mapsto \\mathbb{R}\\) for which we want to compute it‚Äôs dual representation \\(\\hat{F}: \\hat{\\mathcal{X}} \\mapsto \\mathbb{R}\\) under the Legendre Transform. The Legendre transform \\(\\mathcal{T}\\) of \\(F\\) transforms both the function \\(F\\) and its argument \\(\\mathbf{x}\\) into a dual formulation \\(\\hat{F}\\) and \\(\\hat{\\mathbf{x}} = \\sigma(\\mathbf{x}) = \\nabla F(\\mathbf{x})\\). The transform is defined as:\n\\[\n\\hat{F}(\\hat{\\mathbf{x}}) = \\langle \\mathbf{x}, \\hat{\\mathbf{x}} \\rangle - F(\\mathbf{x}).\n\\]\nNote that \\(\\hat{F}\\) is only a function of \\(\\hat{\\mathbf{x}}\\) (\\(\\mathbf{x}\\) is computed as \\(\\mathbf{x} = \\sigma^{(-1)}(\\hat{\\mathbf{x}})\\). You can confirm this for yourself by trying to compute \\(\\frac{\\partial \\hat{F}}{\\partial \\mathbf{x}}\\) and checking that the answer is \\(0\\)).\nThe code for the Legendre transform is easy to implement in jax as a higher order function. We‚Äôll assume that we always have the original variable \\(\\mathbf{x}\\) so that we don‚Äôt need to compute \\(\\sigma^{(-1)}\\).\n\n\nlegendre_transform\n\n legendre_transform (F:Callable)\n\nTransform scalar F(x) into the dual Fhat(xhat, x) using the Legendre transform\n\n\n\n\nType\nDetails\n\n\n\n\nF\nCallable\nThe function to transform\n\n\n\n\n\nExported source\ndef legendre_transform(\n    F: Callable # The function to transform\n    ):\n    \"Transform scalar F(x) into the dual Fhat(xhat, x) using the Legendre transform\"\n\n    # We define custom gradient rules to give jax some autograd shortcuts\n    @jax.custom_jvp\n    def Fhat(xhat, x): return jnp.multiply(xhat, x).sum() - F(x)\n\n    @Fhat.defjvp\n    def Fhat_jvp(primals, tangents):\n        (xhat, x), (dxhat, dx) = primals, tangents\n        o, do = Fhat(xhat, x), jnp.multiply(x, dxhat).sum()\n        return o, do\n\n    return Fhat\n\n\n\n\n\n\n\n\nTest the Legendre transform\n\n\n\n\n\nLet‚Äôs test if the legendre_transform automatic gradients are what we expect:\n\n\nTest the Legendre transform\nx = jnp.array([1., 2, 3])\nF = lagr_sigmoid\ng = jax.nn.sigmoid(x)\nFhat = legendre_transform(F)\n\nassert jnp.allclose(jax.grad(Fhat)(g, x), x)\nassert jnp.all(jax.grad(Fhat, argnums=1)(g, x) == 0.)\n\nx = jnp.array([1., 2, 3])\nF = lagr_identity\ng = x\nFhat = legendre_transform(F)\n\nassert jnp.allclose(jax.grad(Fhat)(g, x), x)\nassert jnp.all(jax.grad(Fhat, argnums=1)(g, x) == 0.)\n\n\n\n\n\nThe Legendre transform is the final piece of the puzzle to describe the energy of a neuron layer.\n\n\n\nNeuronLayer.energy\n\n NeuronLayer.energy (xhat, x)\n\nThe energy of the neuron layer is the Legendre transform of the Lagrangian\n\n\nExported source\n@patch\ndef energy(self: NeuronLayer, xhat, x): \n    \"\"\"The energy of the neuron layer is the Legendre transform of the Lagrangian\"\"\"\n    return legendre_transform(self.lagrangian)(xhat, x)\n\n\nThe energy is the Legendre transform of the Lagrangian:\n\\[\nE_\\text{neuron} = \\langle \\mathbf{x}, \\hat{\\mathbf{x}} \\rangle - \\mathcal{L}_x(\\mathbf{x})\n\\]\n\n\nTest the energy\nnn = NeuronLayer(lagrangian=lagr_sigmoid, shape=(3,))\nx = jnp.array([1., 2, 3])\nxhat = nn.activations(x)\nassert jnp.allclose(jax.grad(nn.energy, argnums=0)(xhat, x), x)\nassert jnp.allclose(jax.grad(nn.energy, argnums=1)(xhat, x), 0.)\n\n\n\n\n\n\n\n\nAdditional methods\n\n\n\n\n\nWe alias a few of the methods for convenience.\n\n\nExported source\nNeuronLayer.sigma = NeuronLayer.activations\nNeuronLayer.E = NeuronLayer.energy\n\n\nAnd wrap a few other python conveniences.\n\n\nNeuronLayer.__post_init__\n\n NeuronLayer.__post_init__ ()\n\nEnsure the neuron shape is a tuple\n\n\n\nNeuronLayer.__repr__\n\n NeuronLayer.__repr__ ()\n\nLook nice when inspected",
    "crumbs": [
      "Building HAMUX"
    ]
  },
  {
    "objectID": "core.html#sec-hypersynapse-implementation",
    "href": "core.html#sec-hypersynapse-implementation",
    "title": "Building HAMUX",
    "section": "Hypersynapse implementation",
    "text": "Hypersynapse implementation\n\nHypersynapses are just callable equinox.Module with trainable parameters. Any differentiable, scalar-valued function, implemented in a __call__ method, will work.\n\nThat‚Äôs it. Many things can be hypersynapse energies. Here are two examples that may look familiar to those with a background in ML.\n\n\nLinearSynapse\n\n LinearSynapse (W:jax.Array)\n\nThe energy synapse corrolary of the linear layer in standard neural networks\n\n\nExported source\nclass LinearSynapse(eqx.Module):\n    \"\"\"The energy synapse corrolary of the linear layer in standard neural networks\"\"\"\n    W: jax.Array\n    def __call__(self, xhat1:jax.Array, xhat2:jax.Array):\n        \"Compute the interaction energy between activations `xhat1` and `xhat2`.\"\n        # Best to use batch-dim agnostic operations\n        return -jnp.einsum(\"...c,...d,cd-&gt;...\", xhat1, xhat2, self.W)\n\n    @classmethod\n    def rand_init(cls, key: jax.Array, x1_dim: int, x2_dim: int):\n        Winit = 0.02 * jr.normal(key, (x1_dim, x2_dim))\n        return cls(W=Winit)\n\n\nTake the gradient w.r.t. either of the input activations and you have a linear layer.\n\nsyn = LinearSynapse.rand_init(jr.key(0), 10, 20)\nxhat1 = jr.normal(jr.key(1), (10,))\nxhat2 = jr.normal(jr.key(2), (20,))\n\nprint(\"Energy:\", syn(xhat1, xhat2))\nff_compute = -jnp.einsum(\"...c,cd-&gt;...d\", xhat1, syn.W)\nfb_compute = -jnp.einsum(\"...d,cd-&gt;...c\", xhat2, syn.W)\nassert jnp.allclose(jax.grad(syn, argnums=1)(xhat1, xhat2), ff_compute)\nassert jnp.allclose(jax.grad(syn, argnums=0)(xhat1, xhat2), fb_compute)\n\nEnergy: 0.045816228\n\n\nThe linear layer is trying to align the activations of its two connected layers, and taking the gradient w.r.t. either of the activations gives you the standard linear layer output.\nWe may want to add biases to the network. We can do so in two ways.\n\n\n\nBiasSynapse\n\n BiasSynapse (b:jax.Array)\n\nEnergy defines constant input to a neuron layer\n\n\n\nLinearSynapseWithBias\n\n LinearSynapseWithBias (W:jax.Array, b:jax.Array)\n\nA linear synapse with a bias\n\nD1, D2 = 10, 20\nW = 0.02 * jr.normal(jr.key(0), (D1, D2))\nb = jnp.arange(D2)+1\nlinear_syn_with_bias = LinearSynapseWithBias(W, b)\n\n# Gradients match how linear layers work\nxhat1 = jr.normal(jr.key(3), (D1,))\nxhat2 = jr.normal(jr.key(4), (D2,))\n\nexpected_forward = W.T @ xhat1 + b\nexpected_backward = W @ xhat2\n\nforward_signal = -jax.grad(linear_syn_with_bias, argnums=1)(xhat1, xhat2)\nbackward_signal = -jax.grad(linear_syn_with_bias, argnums=0)(xhat1, xhat2)\nassert jnp.allclose(forward_signal, expected_forward)\nassert jnp.allclose(backward_signal, expected_backward)\n\n# Could also use a dedicated bias synapse\nbias_syn = BiasSynapse(b=b)\nassert jnp.allclose(-jax.grad(bias_syn)(xhat2), bias_syn.b)\n\nFinally, we can consider even convolutional synapses. We have to get a bit creative to define the energy here to use efficient forward convolution implementations in jax.\n\n\n\nConvSynapse\n\n ConvSynapse (W:jax.Array, window_strides:Tuple[int,int], padding:str)\n\nThe energy corrolary of a convolutional layer in standard neural networks\nThe gradient w.r.t. xhat2 is what we call a ‚Äúforward convolution‚Äù. The gradient w.r.t. xhat1 is a ‚Äútransposed convolution‚Äù!\n\n\nSimple test for ConvSynapse\nkey = jr.key(42)\nH, W, C_in, C_out = 8, 8, 3, 5\nfilter_h, filter_w = 3, 3\n\n# Create a ConvSynapse\nconv_syn = ConvSynapse.from_conv_params(\n    key=key,\n    channels_out=C_out,\n    channels_in=C_in, \n    filter_shape=(filter_h, filter_w),\n    window_strides=(1, 1),\n    padding=\"SAME\"\n)\n\n# Create test activations\nxhat1 = jr.normal(jr.key(1), (H, W, C_in))  # Input activation\nxhat2 = jr.normal(jr.key(2), (H, W, C_out))  # Output activation\n\n# Test energy computation\nenergy = conv_syn(xhat1, xhat2)\nprint(f\"Energy: {energy}\")\nassert isinstance(energy, jax.Array) and energy.shape == ()\n\n# The negative gradient w.r.t. xhat2 is a standard convolution\nconv_result = conv_syn.forward_conv(xhat1[None])[0]  # Remove batch dim\ngrad_xhat2 = jax.grad(conv_syn, argnums=1)(xhat1, xhat2)\nassert jnp.allclose(-grad_xhat2, conv_result)\n\n# Test that the conv_transpose is the same as the gradient w.r.t. xhat1\nconv_transpose_result = jax.lax.conv_transpose(xhat2[None], conv_syn.W, strides=conv_syn.window_strides, padding=conv_syn.padding, dimension_numbers=(\"NHWC\", \"OIHW\", \"NHWC\"), transpose_kernel=True)[0]\ngrad_xhat1 = jax.grad(conv_syn, argnums=0)(xhat1, xhat2)\nassert jnp.allclose(conv_transpose_result, -grad_xhat1)\n\n\nEnergy: -8.785972595214844",
    "crumbs": [
      "Building HAMUX"
    ]
  },
  {
    "objectID": "core.html#energy-hypergraph-implementation",
    "href": "core.html#energy-hypergraph-implementation",
    "title": "Building HAMUX",
    "section": "Energy Hypergraph Implementation",
    "text": "Energy Hypergraph Implementation\nThe local, summing structure of the \\(E^\\text{total}\\) is expressible in code as a hypergraph. We roll our own implementation in JAX to keep things simple.\n\n\nHAM\n\n HAM (neurons:Dict[str,__main__.NeuronLayer],\n      hypersynapses:Dict[str,equinox._module.Module],\n      connections:List[Tuple[Tuple,str]])\n\nA Hypergraph wrapper connecting all dynamic states (neurons) and learnable parameters (synapses) for our associative memory\n\n\nExported source\nclass HAM(eqx.Module): \n    \"A Hypergraph wrapper connecting all dynamic states (neurons) and learnable parameters (synapses) for our associative memory\" \n    neurons: Dict[str, NeuronLayer]\n    hypersynapses: Dict[str, eqx.Module] \n    connections: List[Tuple[Tuple, str]]\n\n\nWe describe an HAM using plain python datastructures for our neurons, hypersynapses and edge list of connections. This makes each object fully compatible with jax‚Äôs tree mapping utilities, which will help keep our hypergraph code super succinct.\nFor example, we can create a simple HAM with two neurons and one hypersynapse:\n\nn1_dim, n2_dim = 10, 100\nneurons = {\n    \"n1\": NeuronLayer(lagrangian=lagr_sigmoid, shape=(n1_dim,)),\n    \"n2\": NeuronLayer(lagrangian=lambda x: lagr_softmax(x, axis=-1), shape=(n2_dim,)),\n}\nhypersynapses = {\n    \"s1\": LinearSynapse.rand_init(jax.random.key(0), n1_dim, n2_dim),\n}\nconnections = [\n    ((\"n1\", \"n2\"), \"s1\"), # Read as: \"Connect neurons n1 and n2 via synapse s1\"\n]\nham = HAM(neurons, hypersynapses, connections)\n\nLet‚Äôs start with some basic description of the hypergraph, describing the data object we want to create.\n\n\n\nHAM.n_connections\n\n HAM.n_connections ()\n\nTotal number of connections\n\n\nExported source\n@patch(as_prop=True)\ndef n_neurons(self:HAM) -&gt; int:\n   \"Total number of neurons\"\n   return len(self.neurons)\n\n@patch(as_prop=True)\ndef n_hypersynapses(self:HAM) -&gt; int:\n   \"Total number of hypersynapses\"\n   return len(self.hypersynapses)\n\n@patch(as_prop=True)\ndef n_connections(self:HAM) -&gt; int:\n   \"Total number of connections\"\n   return len(self.connections)\n\n\n\n\n\nHAM.n_hypersynapses\n\n HAM.n_hypersynapses ()\n\nTotal number of hypersynapses\n\n\n\nHAM.n_neurons\n\n HAM.n_neurons ()\n\nTotal number of neurons\n\n\n\nHAM.init_states\n\n HAM.init_states (bs:Optional[int]=None)\n\nInitialize all neuron states\n\n\nExported source\n@patch\ndef init_states(self: HAM, bs: Optional[int] = None):\n    \"\"\"Initialize all neuron states\"\"\"\n    if bs is not None and bs &gt; 0: warn(\"Vectorize with `ham.vectorize()` before processing batched states\")\n    xs = {k: v.init(bs) for k, v in self.neurons.items()}\n    return xs\n\n\nInitialize all the dynamic neuron states at once, optionally with a batch size. This makes it easy to treat the whole collection of neuron states as a single tensor.\n\nxs = ham.init_states()\nprint(jtu.tree_map(lambda x: f\"Shape: {x.shape}\", xs))\n\n{'n1': 'Shape: (10,)', 'n2': 'Shape: (100,)'}\n\n\nKey into these empty states to replace the states with real data.\n\nexample_data = jr.normal(jr.key(4), xs['n1'].shape)\nxs[\"n1\"] = example_data\n\n\n\n\n\n\n\nVariable naming conventions\n\n\n\nThroughout this code, we universally use the xs variable to refer to the collection of neuron internal states and the xhats variable to refer to the collection of neuron activations.\nAdditionally, whenever a function f takes both xs and xhats as arguments, we assume the xhats are passed first in the argument order i.e., f(xhats, xs, *args, **kwargs). This is because most AM operations do gradient descent on the activations, not the internal states, and the 0-th positional arg is the default argument for jax.grad\n\n\n\n\n\nHAM.activations\n\n HAM.activations (xs)\n\nConvert hidden states of each neuron into their activations\n\n\nExported source\n@patch\ndef activations(self: HAM, xs):\n    \"\"\"Convert hidden states of each neuron into their activations\"\"\"\n    xhats = {k: v.sigma(xs[k]) for k, v in self.neurons.items()}\n    return xhats\n\n\nFrom the states, we can compute the activations of each neuron as a single collection:\n\nxhats = ham.activations(xs)\nassert jnp.all(xhats['n1'] &gt; 0) and jnp.all(xhats['n1'] &lt; 1), \"Sigmoid neurons should be between 0 and 1\"\nassert jnp.isclose(xhats['n2'].sum(), 1.0), \"Softmax neurons should sum to 1\"\n\n\n\n\nHAM.energy\n\n HAM.energy (xhats, xs)\n\nThe complete energy of the HAM\n\n\n\nHAM.energy_tree\n\n HAM.energy_tree (xhats, xs)\n\nReturn energies for each individual component\n\n\n\nHAM.connection_energies\n\n HAM.connection_energies (xhats)\n\nGet the energy for each connection\n\n\n\nHAM.neuron_energies\n\n HAM.neuron_energies (xhats, xs)\n\nRetrieve the energies of each neuron in the HAM\nFrom the activations, we can collect all the energies of the neurons and the connections in the HAM. We can organize these into an energy tree from which we compute the total energy of the entire HAM..\nThe complete energy of the HAM is the sum of all the individual energies from the HAM.energy_tree.\n\nxhats = ham.activations(xs)\npp(ham.energy_tree(xhats, xs))\n\n{'neurons': {'n1': Array(-6.215218, dtype=float32),\n             'n2': Array(-4.6051702, dtype=float32)},\n 'connections': [Array(-0.00045318, dtype=float32)]}\n\n\n\npp(ham.energy(xhats, xs))\n\nArray(-10.820841, dtype=float32)\n\n\n\n\n\nHAM.dEdact\n\n HAM.dEdact (xhats, xs, return_energy=False)\n\nCalculate gradient of system energy w.r.t. each activation\nA small helper function to make it easier to compute the gradient of the energy w.r.t. the activations.\nThis energy is guaranteed to monotonically decrease over time, and be bounded from below.",
    "crumbs": [
      "Building HAMUX"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HAMUX",
    "section": "",
    "text": "A universal language for describing Associative Memories as hypergraphs\nA new class of Deep Learning model built around ENERGY.\nPart proof-of-concept, part functional prototype, HAMUX is designed to bridge modern AI architectures and Hopfield Networks.\nHAMUX: A Hierarchical Associative Memory User eXperience.\nDocumentation. Also described in our tutorial on Associative Memory (ch.¬†3)."
  },
  {
    "objectID": "index.html#a-universal-abstraction-for-hopfield-networks",
    "href": "index.html#a-universal-abstraction-for-hopfield-networks",
    "title": "HAMUX",
    "section": "A Universal Abstraction for Hopfield Networks",
    "text": "A Universal Abstraction for Hopfield Networks\nHAMUX fully captures the the energy fundamentals of Hopfield Networks and enables anyone to:\n\nüß† Build DEEP Hopfield nets\nüß± With modular ENERGY components\nüèÜ That resemble modern DL operations\n\nEvery architecture built using HAMUX is a formal Associative Memory (AM). That is, the architecture defines a tractable energy, whose minimization describes a dynamical system that is guaranteed to converge to a fixed point. Hierarchical Associative Memories (HAMs) have several additional advantages over traditional Hopfield Networks (HNs):\n\n\n\n\n\n\n\nHopfield Networks (HNs)\nHierarchical Associative Memories (HAMs)\n\n\n\n\nHNs are only two layers systems\nHAMs connect any number of layers\n\n\nHNs have limited storage capacity\nHAMs can be used to describe Associative Memories with much denser storage\n\n\nHNs model only simple relationships between layers\nHAMs model any complex but differentiable operation (e.g., convolutions, pooling, attention, \\(\\ldots\\))\n\n\nHNs use only pairwise synapses\nHAMs can also use many-body synapses (which we denote HyperSynapses)"
  },
  {
    "objectID": "index.html#how-does-hamux-work",
    "href": "index.html#how-does-hamux-work",
    "title": "HAMUX",
    "section": "How does HAMUX work?",
    "text": "How does HAMUX work?\n\n\n\nHAMUX is a hypergraph of üåÄneurons connected via ü§ùhypersynapses, an abstraction sufficiently general to model any conceivable Associative Memory.\n\n\nSee our walkthrough in this notebook for a more detailed explanation of how everything works.\nIn summary, this library handles all the complexity of scaling modular, learnable energy functions that interconnect many layers and hypersynapses. It is a barebones framework to explore Associative Memories that look like Deep Learning architectures.\n\nImplement your favorite Deep Learning operations as a HyperSynapse\nPort over your favorite activation functions as Lagrangians\nConnect layers and hypersynapses into a hypergraph with a single total energy.\nEasily use autograd for descending states.\n\nAll of this made possible by JAX and equinox."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "HAMUX",
    "section": "Installation",
    "text": "Installation\nInstall latest from the GitHub repo\n$ pip install git+https://github.com/bhoov/hamux.git\nor from pypi\npip install hamux"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "HAMUX",
    "section": "How to use",
    "text": "How to use\n\nimport hamux as hmx\nimport jax, jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu\nimport equinox as eqx\nfrom typing import *\nimport matplotlib.pyplot as plt\n\n\nclass LinearSynapse(eqx.Module):\n    \"\"\"The energy synapse corrolary of the linear layer in standard neural networks\"\"\"\n    W: jax.Array\n    def __call__(self, xhat1:jax.Array, xhat2:jax.Array):\n        return xhat1 @ self.W @ xhat2\n\n    @classmethod\n    def rand_init(cls, key: jax.Array, D1: int, D2: int):\n        Winit = 0.02 * jr.normal(key, (D1, D2))\n        return cls(W=Winit)\n\nkey = jr.key(0)\nnhid = 9\nnlabel = 8\nninput = 7\n\nneurons = {\n    \"input\": hmx.NeuronLayer(hmx.lagr_identity, (ninput,)),\n    \"labels\": hmx.NeuronLayer(hmx.lagr_softmax, (nlabel,)),\n    \"hidden\": hmx.NeuronLayer(hmx.lagr_softmax, (nhid,))\n}\n\nsynapses = {\n    \"dense1\": LinearSynapse.rand_init(key, ninput, nhid),\n    \"dense2\": LinearSynapse.rand_init(key, nlabel, nhid)\n}\n\nconnections = [\n    ((\"input\", \"hidden\"), \"dense1\"),\n    ((\"labels\", \"hidden\"), \"dense2\")\n]\n\nham = hmx.HAM(neurons, synapses, connections)\n\n\nxs = ham.init_states() # No batch size\nxhats = ham.activations(xs)\n\nham.energy_tree(xhats, xs)\nham.energy(xhats, xs)\nham.dEdact(xhats, xs)\n\n{'hidden': Array([ 0.00705259,  0.00320656, -0.02189678,  0.00424237,  0.00248319,\n        -0.00192548,  0.00498188,  0.00388546,  0.00415148], dtype=float32),\n 'input': Array([ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904], dtype=float32),\n 'labels': Array([ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904,  0.00254421], dtype=float32)}\n\n\n\nvham = ham.vectorize()\nbs = 3\nxs = vham.init_states(bs) # Batch size 3\nxhats = vham.activations(xs)\n\nprint(vham.energy_tree(xhats, xs))\nprint(vham.energy(xhats, xs))\nprint(vham.dEdact(xhats, xs))\n\nham = vham.unvectorize()\n\n{'connections': [Array([0., 0., 0.], dtype=float32), Array([0.00068681, 0.00068681, 0.00068681], dtype=float32)], 'neurons': {'hidden': Array([-2.1972246, -2.1972246, -2.1972246], dtype=float32), 'input': Array([0., 0., 0.], dtype=float32), 'labels': Array([-2.0794415, -2.0794415, -2.0794415], dtype=float32)}}\n[-4.275979 -4.275979 -4.275979]\n{'hidden': Array([[ 0.00705259,  0.00320656, -0.02189678,  0.00424237,  0.00248319,\n        -0.00192548,  0.00498188,  0.00388546,  0.00415148],\n       [ 0.00705259,  0.00320656, -0.02189678,  0.00424237,  0.00248319,\n        -0.00192548,  0.00498188,  0.00388546,  0.00415148],\n       [ 0.00705259,  0.00320656, -0.02189678,  0.00424237,  0.00248319,\n        -0.00192548,  0.00498188,  0.00388546,  0.00415148]],      dtype=float32), 'input': Array([[ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904],\n       [ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904],\n       [ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904]], dtype=float32), 'labels': Array([[ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904,  0.00254421],\n       [ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904,  0.00254421],\n       [ 0.00667361,  0.00921866,  0.00110246, -0.00476699, -0.0013505 ,\n        -0.00371795, -0.00420904,  0.00254421]], dtype=float32)}\n\n\nWe can check that the energy descent using randomize weights behaves as expected.\n\n# Randomize the initial states\nrkey = jr.key(42)\nkey_tree = dict(zip(xs.keys(), jr.split(rkey, len(xs))))\nxs = jtu.tree_map(lambda key, x: jr.normal(key, x.shape), key_tree, xs)\nxhats = vham.activations(xs)\n\nnsteps = 40\nstep_size = 0.5\nenergies = jnp.empty((nsteps, bs))\nfor i in range(nsteps):\n    energy, dEdxhats = vham.dEdact(xhats, xs, return_energy=True)\n    energies = energies.at[i].set(energy)\n    xs = jtu.tree_map(lambda x, u: x - step_size * u, xs, dEdxhats)\n\nplt.plot(jnp.arange(nsteps), energies)\n\n\n\n\n\n\n\n\nSee the nbs/_examples directory for more examples."
  },
  {
    "objectID": "index.html#developing-locally",
    "href": "index.html#developing-locally",
    "title": "HAMUX",
    "section": "Developing locally",
    "text": "Developing locally\nuv (in pyproject.toml) handles all dependencies, nbdev (and its settings.ini) handles all packaging. We handle syncing between the pyproject.toml and settings.ini files using scripts/sync_dependencies.py.\n\n\n\n\n\n\nWarning\n\n\n\nPackage is currently based on a fork of nbdev that allows development in plain text .qmd files.\n\n\nPrerequisite: Download ‚Äòuv‚Äô\nuv sync\nuv run uv pip install -e .\n\n# OPTIONAL: Add GPU enabled JAX e.g., for CUDA 12\nuv run uv pip install -U \"jax[cuda12]\"\n\nsource .venv/bin/activate\nnbdev_prepare\nuv sync\nsource .venv/bin/activate\n\n# Make changes to source files in `nbs/`.\nuv run nbdev_prepare # Before committing changes, export and test library\nuv run nbdev_preview # Preview docs\n\nVSCode for developmentautomatic library export\n\nNever let your .qmd source get out of sync with your .py library.\n\nVSCode has an excellent interactive mode for developing quarto files. We install the Run on Save extension to keep the .qmd files in sync with the .py library, removing the need for explicit nbdev_export commands.\nTo accomplish this, copy and paste the following into your user/workspace settings (Cmd+Shift+P then either ‚ÄúPreferences: Open User settings (JSON)‚Äù or ‚ÄúPreferences: Open Workspace settings (JSON)‚Äù)\n{\n    \"files.watcherExclude\": {\n        \"**/.git/objects/**\": true,\n        \"**/.git/subtree-cache/**\": true,\n        \"**/node_modules/*/**\": true,\n        \"**/.hg/store/**\": true,\n    },\n    \"emeraldwalk.runonsave\": {\n        \"commands\": [\n        {\n            \"match\": \"nbs/.*\\\\.qmd$\", // Replace with your own nbs/ directory\n            \"cmd\": \"source ${workspaceFolder}/.venv/bin/activate && nbdev_export\", // Replace with a path to your python env where `nbdev` is installed\n        }\n        ]\n    }\n}"
  }
]