{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464acfcb-4214-4646-8ae8-348d3a596e6c",
   "metadata": {},
   "source": [
    "# Converting to TFJS\n",
    "\n",
    "> Package and ship to the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26cac9-25d4-4044-89f5-aa6d0ccbf78f",
   "metadata": {},
   "source": [
    "One appeal of HAMs is their general compactness -- all model weights are symmetrical, so powerful models can more easily fit inside RAM. In addition, we believe HAMs to be more interpretable and fun to play around with. For these two reasons, we are building some simple helper functions to convert trained HAM models into the frontend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e3694-f799-4aa4-83b7-4a323817d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tfjs_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8503e8-3a96-4cd3-802e-83d288a6f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2055d-603e-491f-a5b8-15857644b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79400474-fe9a-42cc-ae5e-78feb69831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import tensorflowjs as tfjs\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "from tensorflowjs.converters import tf_saved_model_conversion_v2 as saved_model_conversion\n",
    "from jax.experimental import jax2tf\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d562efc-b08e-4f22-b62d-720fce957236",
   "metadata": {},
   "source": [
    "The below code is primarily copied from the source code present in the examples in the [official tutorial](https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a944ce-3fe4-4e21-942d-055435dc9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DType = Any\n",
    "PolyShape = jax2tf.shape_poly.PolyShape\n",
    "Array = Any \n",
    "_TF_SERVING_KEY = tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61996774-e0d8-4265-a2b1-4016dd951647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _ReusableSavedModelWrapper(tf.train.Checkpoint):\n",
    "    \"\"\"Wraps a function and its parameters for saving to a SavedModel.\n",
    "    Implements the interface described at\n",
    "    https://www.tensorflow.org/hub/reusable_saved_models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tf_graph, param_vars):\n",
    "        \"\"\"Args:\n",
    "          tf_graph: a tf.function taking one argument (the inputs), which can be\n",
    "             be tuples/lists/dictionaries of np.ndarray or tensors. The function\n",
    "             may have references to the tf.Variables in `param_vars`.\n",
    "          param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable,\n",
    "             to be saved as the variables of the SavedModel.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models\n",
    "        self.variables = tf.nest.flatten(param_vars)\n",
    "        self.trainable_variables = [v for v in self.variables if v.trainable]\n",
    "        # If you intend to prescribe regularization terms for users of the model,\n",
    "        # add them as @tf.functions with no inputs to this list. Else drop this.\n",
    "        self.regularization_losses = []\n",
    "        self.__call__ = tf_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744561e-43f7-43fd-83e6-1264630bb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_jax(\n",
    "    apply_fn: Callable[..., Any],\n",
    "    *,\n",
    "    input_signatures: Sequence[Tuple[Sequence[Union[int, None]], DType]],\n",
    "    model_dir: str,\n",
    "    polymorphic_shapes: Optional[Sequence[Union[str, PolyShape]]] = None):\n",
    "    \"\"\"Converts a JAX function `apply_fn` to a TensorflowJS model. \n",
    "    Works with `functools.partial` style models if we don't need to access the variables in the frontend.\n",
    "\n",
    "    Example usage for an arbitrary function:\n",
    "\n",
    "    ```\n",
    "    import functools as ft\n",
    "    ...\n",
    "    def predict_fn(model, input):\n",
    "        return model.predict(input)\n",
    "\n",
    "    fn = ft.partial(predict_fn, trained_model)\n",
    "\n",
    "    convert_jax(\n",
    "        apply_fn=fn,\n",
    "        input_signatures=[((D1, D2,), np.float32)],\n",
    "        model_dir=tfjs_model_dir)\n",
    "    ```\n",
    "\n",
    "    Note that when using dynamic shapes, an additional argument\n",
    "    `polymorphic_shapes` should be provided specifying values for the dynamic\n",
    "    (\"polymorphic\") dimensions). See here for more details:\n",
    "    https://github.com/google/jax/tree/main/jax/experimental/jax2tf#shape-polymorphic-conversion\n",
    "\n",
    "    This is an adaption of the original implementation in jax2tf here:\n",
    "    https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/saved_model_lib.py\n",
    "\n",
    "    Arguments:\n",
    "    apply_fn: A JAX function that has one or more arguments, of which the first\n",
    "      argument are the model parameters. This function typically is the forward\n",
    "      pass of the network (e.g., `Module.apply()` in Flax).\n",
    "    input_signatures: the input signatures for the second and remaining\n",
    "      arguments to `apply_fn` (the input). A signature must be a\n",
    "      `tensorflow.TensorSpec` instance, or a (nested) tuple/list/dictionary\n",
    "      thereof with a structure matching the second argument of `apply_fn`.\n",
    "    model_dir: Directory where the TensorflowJS model will be written to.\n",
    "    polymorphic_shapes: If given then it will be used as the\n",
    "      `polymorphic_shapes` argument for the second parameter of `apply_fn`. In\n",
    "      this case, a single `input_signatures` is supported, and should have\n",
    "      `None` in the polymorphic (dynamic) dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_fn = jax2tf.convert(\n",
    "        apply_fn,\n",
    "        # Gradients must be included as 'PreventGradient' is not supported.\n",
    "        with_gradient=True,\n",
    "        polymorphic_shapes=polymorphic_shapes,\n",
    "        # Do not use TFXLA Ops because these aren't supported by TFjs, but use\n",
    "        # workarounds instead. More information:\n",
    "        # https://github.com/google/jax/tree/main/jax/experimental/jax2tf#tensorflow-xla-ops\n",
    "        enable_xla=False)\n",
    "\n",
    "    # Create tf.Variables for the parameters. If you want more useful variable\n",
    "    # names, you can use `tree.map_structure_with_path` from the `dm-tree`\n",
    "    # package.\n",
    "    # For HAMUX we presume that the variables are inaccessible, for now\n",
    "    param_vars = []\n",
    "    # param_vars = tf.nest.map_structure(\n",
    "    #     lambda param: tf.Variable(param, trainable=False), params)\n",
    "    # Do not use TF's jit compilation on the function.\n",
    "    tf_graph = tf.function(\n",
    "        lambda *xs: tf_fn(*xs), autograph=False, jit_compile=False)\n",
    "\n",
    "    # This signature is needed for TensorFlow Serving use.\n",
    "    signatures = {\n",
    "        _TF_SERVING_KEY: tf_graph.get_concrete_function(*input_signatures)\n",
    "    }\n",
    "\n",
    "    wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars)\n",
    "    saved_model_options = tf.saved_model.SaveOptions(\n",
    "        experimental_custom_gradients=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as saved_model_dir:\n",
    "        tf.saved_model.save(\n",
    "            wrapper,\n",
    "            saved_model_dir,\n",
    "            signatures=signatures,\n",
    "            options=saved_model_options)\n",
    "        saved_model_conversion.convert_tf_saved_model(saved_model_dir, model_dir, skip_op_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b71f8-6089-45ef-a626-b8753693c7bf",
   "metadata": {},
   "source": [
    "Let's presume the following example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d0721-26e9-4d16-a82b-9f98999a4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hamux as hmx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import functools as ft\n",
    "\n",
    "model, fwd = hmx.create_model(\"hn_softmax_mnist\")\n",
    "states, model = model.init_states_and_params(jax.random.PRNGKey(0), bs=1)\n",
    "\n",
    "def simple_batch_fwd(\n",
    "    x: jnp.ndarray, # Starting point for clamped layer[0]\n",
    "    dt: float): # Step size through time\n",
    "    \"\"\"A simple version of the forward function\"\"\"\n",
    "    # Initialize hidden states to our image\n",
    "    xs = model.init_states(x.shape[0])\n",
    "    xs[0] = jnp.array(x)\n",
    "\n",
    "    updates = model.vupdates(xs)  # Calculate the updates\n",
    "    new_xs = model.step(\n",
    "        xs, updates, dt=dt\n",
    "    )  # Add them to our current states\n",
    "\n",
    "    # All labels have a softmax activation function as the last layer, spitting out probabilities\n",
    "    return model.layers[-1].g(xs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f75682-f6cf-4aa4-a6f5-f49fe58d5327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 17:02:40.458245: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weight file _archive/hamux_model/model.json...\n"
     ]
    }
   ],
   "source": [
    "# specify where to save the model\n",
    "tfjs_model_dir = f'_archive/hamux_model/'\n",
    "convert_jax(\n",
    "    simple_batch_fwd,\n",
    "    input_signatures=[tf.TensorSpec(states[0].shape, tf.float32), tf.TensorSpec((1,), tf.float32)], # img, dt\n",
    "    model_dir=tfjs_model_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
