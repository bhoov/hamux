{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAM\n",
    "\n",
    "> Assembling layers and synapses into a single system governed by an energy function\n",
    "\n",
    "We have now provided the two primary components: `layers` and `synapses`. This module assembles those together into a single network that is governed by an energy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import *\n",
    "import treex as tx\n",
    "from hamux.layers import Layer\n",
    "from hamux.synapses import Synapse\n",
    "import jax.tree_util as jtu\n",
    "from hamux.utils import pytree_save, pytree_load, to_pickleable, align_with_state_dict\n",
    "import pickle\n",
    "import functools as ft\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HAM System\n",
    "\n",
    "We connect `layers` and `synapses` in a [hypergraph](https://en.wikipedia.org/wiki/Hypergraph) to describe the energy function. A hypergraph is a generalization of the familiar graph in that edges (synapses) can connect multiple nodes (layers). This graph -- complete with the operations of the synapses and the activation behavior of the layers -- fully defines the energy function for a given collection of neuron states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HAM(tx.Module):\n",
    "    \"\"\"Connecting neuron layers and synapses into a hypergraph\"\"\"\n",
    "    layers: List[Layer]\n",
    "    synapses: List[Synapse]\n",
    "    connections: List[Tuple[Tuple, int]]\n",
    "\n",
    "    def __init__(self, layers, synapses, connections):\n",
    "        self.layers = layers\n",
    "        self.synapses = synapses\n",
    "        self.connections = connections\n",
    "\n",
    "    @property\n",
    "    def n_layers(self): return len(self.layers)\n",
    "    @property\n",
    "    def n_synapses(self): return len(self.synapses)\n",
    "    @property\n",
    "    def n_connections(self): return len(self.connections)\n",
    "    @property\n",
    "    def layer_taus(self): return [layer.tau for layer in self.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is typical for JAX frameworks, the parameters of HAMs need to be initialized. Unlike other machine learning libraries, the *states* of each *layer* -- that is, the dynamical variables of our system -- also need to be initialized. The notation $\\mathbf{x}$ indicates the collection of all states from each layer, and $x^\\alpha$ indicates that we are referring to the state of layer at index $\\alpha$ in our collection. \n",
    "\n",
    "We provide this functionality with the following helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def init_states(self:HAM, \n",
    "                bs=None, # Batch size of the states to initialize, if needed\n",
    "                rng=None): # RNG seed for random initialization of the states, if non-zero initializations are desired\n",
    "    \"\"\"Initialize the states of every layer in the network\"\"\"\n",
    "    if rng is not None:\n",
    "        keys = jax.random.split(rng, self.n_layers)\n",
    "        return [layer.init_state(bs, rng=key) for layer, key in zip(self.layers, keys)]\n",
    "    return [layer.init_state(bs) for layer in self.layers]\n",
    "\n",
    "@patch\n",
    "def init_states_and_params(self:HAM, \n",
    "                           param_key, # RNG seed for random initialization of the parameters\n",
    "                           bs=None, # Batch size of the states to initialize, if needed\n",
    "                           state_key=None): # RNG seed for random initialization of the states, if non-zero initializations are desired\n",
    "    \"\"\"Initialize the states and parameters of every layer and synapse in the network\"\"\"\n",
    "    # params don't need a batch size to initialize\n",
    "    params = self.init(param_key, self.init_states(), call_method=\"energy\")\n",
    "    states = self.init_states(bs, rng=state_key)\n",
    "    return states, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hamux.layers import *\n",
    "from hamux.synapses import *\n",
    "import hamux.lagrangians as lag\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    IdentityLayer((2,)),\n",
    "    ReluLayer((3,))\n",
    "]\n",
    "\n",
    "synapses = [\n",
    "    DenseSynapse()\n",
    "]\n",
    "\n",
    "connections = [\n",
    "    ((0,1), 0)\n",
    "]\n",
    "xs, ham = HAM(layers, synapses, connections).init_states_and_params(jax.random.PRNGKey(0), state_key=jax.random.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.27703857,  1.351606  ], dtype=float32), array([0.511158  , 2.276133  , 0.23958689], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print([np.array(x) for x in xs]) # The dynamic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceArray([[0.01198053, 0.00434429, 0.01321206],\n",
      "             [0.00065335, 0.0243299 , 0.02388163]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print([s.W for s in ham.synapses]) # The parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy of our whole system is well defined:\n",
    "\n",
    "$$E_\\text{system}(\\mathbf{x}) = E_\\text{layers}(\\mathbf{x}) + E_\\text{synapses}(\\mathbf{g}(\\mathbf{x}))$$\n",
    "\n",
    "where $\\mathbf{x}$ is a collection of the states of our system, and $\\mathbf{g}(\\mathbf{x})$ is an identically shaped collection of the corresponding activations of our system. Then, for any instance at time $t$ we can compute the energy as a function of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def activations(self:HAM, \n",
    "                xs:jnp.ndarray): # Collection of states for each layer\n",
    "    \"\"\"Turn a collection of states into a collection of activations\"\"\"\n",
    "    gs = [self.layers[i].g(xs[i]) for i in range(len(xs))]\n",
    "    return gs\n",
    "\n",
    "@patch\n",
    "def layer_energy(self:HAM,\n",
    "                 xs:jnp.ndarray): # Collection of states for each layer\n",
    "    \"\"\"The total contribution of the layers' contribution to the energy of the HAM\"\"\"\n",
    "    energies = jnp.stack([self.layers[i].energy(x) for i, x in enumerate(xs)])\n",
    "    return jnp.sum(energies)\n",
    "\n",
    "@patch\n",
    "def synapse_energy(self:HAM,\n",
    "                   gs:jnp.ndarray): # Collection of activations of each layer\n",
    "    \"\"\"The total contribution of the synapses' contribution to the energy of the HAM\"\"\"\n",
    "    def get_energy(lset, k):\n",
    "        mygs = [gs[i] for i in lset]\n",
    "        synapse = self.synapses[k]\n",
    "        return synapse.energy(*mygs)\n",
    "    energies = jnp.stack([get_energy(lset, k) for lset, k in self.connections])\n",
    "    return jnp.sum(energies)\n",
    "\n",
    "@patch\n",
    "def energy(self:HAM,\n",
    "           xs:jnp.ndarray): # Collection of states for each layer\n",
    "    \"\"\"The complete energy of the HAM\"\"\"\n",
    "    gs = self.activations(xs)\n",
    "    energy = self.layer_energy(xs) + self.synapse_energy(gs)\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(3.7015278, dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_L = ham.layer_energy(xs); E_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.07772133, dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = ham.activations(xs)\n",
    "E_S = ham.synapse_energy(gs); E_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(ham.energy(xs), E_L+E_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for each of the layer states is simply defined as follows:\n",
    "\n",
    "$$\\tau \\frac{dx^\\alpha}{dt} = -\\frac{dE_\\text{system}}{dg^\\alpha}$$\n",
    "\n",
    "JAX is wonderful. Autograd does this accurately and efficiently for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def dEdg(self:HAM, \n",
    "         xs:jnp.ndarray):\n",
    "    \"\"\"Calculate the gradient of system energy wrt. the activations\n",
    "\n",
    "    Notice that we use an important mathematical property of the Legendre transform to take a mathematical, where dE_layer / dg = x\n",
    "    \"\"\"\n",
    "    gs = self.activations(states)\n",
    "    return jtu.tree_map(\n",
    "        lambda x, s: x + s, xs, jax.grad(self.synapse_energy)(gs)\n",
    "    )\n",
    "\n",
    "@patch\n",
    "def updates(self:HAM,\n",
    "            xs:jnp.ndarray): # Collection of states for each layer\n",
    "    \"\"\"The negative of our dEdg, computing the update direction each layer should descend\"\"\"\n",
    "    return jtu.tree_map(lambda dg: -dg, self.dEdg(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement a simple, stochastic step function, though more advanced optimizations from the JAX ecosystem (e.g., [optax](https://github.com/deepmind/optax)) can easily be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def step(self:HAM,\n",
    "    xs: List[jnp.ndarray], # Collection of current states for each layer\n",
    "    updates: List[jnp.ndarray], # Collection of update directions for each state\n",
    "    dt: float = 0.1, # Stepsize to take in direction of updates\n",
    "    masks: Optional[List[jnp.ndarray]] = None, # Boolean mask, 0 if clamped neuron, and 1 elsewhere. A pytree identical to `xs`. Optional.\n",
    "):\n",
    "    \"\"\"A discrete step down the energy using step size `dt` scaled by the `tau` of each layer\"\"\"\n",
    "    taus = self.layer_taus\n",
    "    alphas = [dt / tau for tau in taus] # Scaling factor of the update size of each layer\n",
    "    if masks is not None:\n",
    "        next_xs = jtu.tree_map(lambda x, u, m, alpha: x + alpha * u * m, xs, updates, masks, alphas)\n",
    "    else:\n",
    "        next_xs = jtu.tree_map(lambda x, u, alpha: x + alpha * u, xs, updates, alphas)\n",
    "    return next_xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is particularly useful if all of these functions can be applied to a batched collection of states, something JAX makes particularly easy through its [`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html) functionality. We prefix vectorized versions of the above methods with a `v`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def _statelist_batch_axes(self:HAM):\n",
    "    \"\"\"A helper function to tell vmap to batch along the 0'th dimension of each state in the HAM.\"\"\"\n",
    "    return ([0 for _ in range(self.n_layers)],)\n",
    "    \n",
    "@patch\n",
    "def vactivations(self:HAM, \n",
    "                 xs: List[jnp.ndarray]): # Collection of states for each layer\n",
    "    \"\"\"A vectorized version of `activations`\"\"\"\n",
    "    return jax.vmap(self.activations, in_axes=self._statelist_batch_axes())(xs)\n",
    "\n",
    "@patch\n",
    "def venergy(self:HAM, \n",
    "            xs: List[jnp.ndarray]): # Collection of states for each layer\n",
    "    \"\"\"A vectorized version of `energy`\"\"\"\n",
    "    return jax.vmap(self.energy, in_axes=self._statelist_batch_axes())(xs)\n",
    "\n",
    "@patch\n",
    "def vdEdg(self:HAM, \n",
    "          xs: List[jnp.ndarray]): # Collection of states for each layer\n",
    "    \"\"\"A vectorized version of `dEdg`\"\"\"\n",
    "    return jax.vmap(self.dEdg, in_axes=self._statelist_batch_axes())(xs)\n",
    "\n",
    "@patch\n",
    "def vupdates(self:HAM,\n",
    "             xs: List[jnp.ndarray]): # Collection of states for each layer\n",
    "    \"\"\"A vectorized version of `updates`\"\"\"\n",
    "    return jax.vmap(self.updates, in_axes=self._statelist_batch_axes())(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide examples here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some helper functions to save and load this model during training. (Needs a better way to save and load the state dict.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def load_state_dict(self:HAM, \n",
    "                    state_dict:Any): # The dictionary of all parameters, saved by `save_state_dict`\n",
    "    if not self.initialized:\n",
    "        _, self = self.init_states_and_params(jax.random.PRNGKey(0), 1)\n",
    "    self.connections = state_dict[\"connections\"]\n",
    "    self.layers = align_with_state_dict(self.layers, state_dict[\"layers\"])\n",
    "    self.synapses = align_with_state_dict(self.synapses, state_dict[\"synapses\"])\n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def save_state_dict(self:HAM, \n",
    "                    fname:Union[str, Path], # Filename of checkpoint to save\n",
    "                    overwrite:bool=True): # Overwrite an existing file of the same name?\n",
    "    to_save = jtu.tree_map(to_pickleable, self.to_dict())\n",
    "    pytree_save(to_save, fname, overwrite=overwrite)\n",
    "\n",
    "@patch\n",
    "def load_ckpt(self:HAM, \n",
    "              ckpt_f:Union[str, Path]): # Filename of checkpoint to load\n",
    "    with open(ckpt_f, \"rb\") as fp:\n",
    "        state_dict = pickle.load(fp)\n",
    "    return self.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdc5f107602d302d3282ec40de923ef553a5ac4d122eec8045c417a16238788c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
