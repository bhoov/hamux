{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrangians\n",
    "\n",
    "> The physics of associative memories is captured in the **Lagrangian** operation of neurons\n",
    "\n",
    "We begin with the lagrangian, the fundamental building block of any neuron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lagrangians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "from fastcore.test import *\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functional interface\n",
    "\n",
    "Here we define our lagrangian functions, which can be thought of as the integrand of common activation functions in Deep Learning literature. All lagrangians are (potentially parameterized) functions of the form:\n",
    "\n",
    "$$\\mathcal{L}(x;\\ldots) \\mapsto \\mathbb{R}$$\n",
    "\n",
    "where $x$ can be a tensor of arbitrary shape. It is important that our Lagrangians be convex and differentiable.\n",
    "\n",
    "We want to rely on JAX's autograd to automatically differentiate our lagrangians into activation functions. In certain cases (e.g., `lagr_sigmoid` and `lagr_tanh`), autodiff will create a numerically unstable activation function. We follow JAX's [documentation guidelines](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html) to define `custom_jvp`s to fix this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lagr_identity(x): return 1 / 2 * jnp.power(x, 2).sum()\n",
    "\n",
    "def lagr_repu(x, \n",
    "              n): # Degree of the polynomial in the power unit\n",
    "    \"\"\"Rectified Power Unit of degree `n`\"\"\"\n",
    "    return 1 / n * jnp.power(jnp.maximum(x, 0), n).sum()\n",
    "\n",
    "def lagr_relu(x):\n",
    "    \"\"\"Rectified Linear Unit. Same as repu of degree 2\"\"\"\n",
    "    return lagr_repu(x, 2)\n",
    "\n",
    "def lagr_softmax(x,\n",
    "                 beta=1.0, # Inverse temperature\n",
    "                 axis=-1): # Dimension over which to apply logsumexp\n",
    "    \"\"\"The lagrangian of the softmax -- the logsumexp\"\"\"\n",
    "    return (1/beta * jax.nn.logsumexp(beta * x, axis=axis, keepdims=True)).sum()\n",
    "\n",
    "def lagr_exp(x, \n",
    "             beta=1.0): # Inverse temperature\n",
    "    \"\"\"Exponential activation function, as in [Demicirgil et al.](https://arxiv.org/abs/1702.01929)\"\"\"\n",
    "    return 1 / beta * jnp.exp(beta * x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lagrangian of the `sigmoid` and the `tanh` are a bit more numerically unstable. We will need to define custom gradients for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jax.custom_jvp\n",
    "def _lagr_sigmoid(x, \n",
    "                  beta=1.0, # Inverse temperature\n",
    "                  scale=1.0): # Amount to stretch the range of the sigmoid's lagrangian\n",
    "    return scale / beta * jnp.log(jnp.exp(beta * x) + 1)\n",
    "\n",
    "def tempered_sigmoid(x, \n",
    "                     beta=1.0, # Inverse temperature\n",
    "                     scale=1.0): # Amount to stretch the range of the sigmoid\n",
    "    \"\"\"The basic sigmoid, but with a scaling factor\"\"\"\n",
    "    return scale / (1 + jnp.exp(-beta * x))\n",
    "\n",
    "@_lagr_sigmoid.defjvp\n",
    "def _lagr_sigmoid_jvp(primals, tangents):\n",
    "    x, beta, scale = primals\n",
    "    x_dot, beta_dot, scale_dot = tangents\n",
    "    primal_out = _lagr_sigmoid(x, beta, scale)\n",
    "    tangent_out = tempered_sigmoid(x, beta=beta, scale=scale) * x_dot # Manually defined sigmoid\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "def lagr_sigmoid(x, \n",
    "                 beta=1.0, # Inverse temperature\n",
    "                 scale=1.0): # Amount to stretch the range of the sigmoid's lagrangian\n",
    "    \"\"\"The lagrangian of the sigmoid activation function\"\"\"\n",
    "    return _lagr_sigmoid(x, beta=beta, scale=scale).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "x = np.random.randn(4,20);beta=0.2; scale=1.3\n",
    "test_eq(tempered_sigmoid(x, beta=beta, scale=scale), jax.grad(ft.partial(lagr_sigmoid, beta=beta, scale=scale))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@jax.custom_jvp\n",
    "def _lagr_tanh(x, beta=1.0):\n",
    "    return 1 / beta * jnp.log(jnp.cosh(beta * x))\n",
    "\n",
    "@_lagr_tanh.defjvp\n",
    "def _lagr_tanh_defjvp(primals, tangents):\n",
    "    x, beta = primals\n",
    "    x_dot, beta_dot = tangents\n",
    "    primal_out = _lagr_tanh(x, beta)\n",
    "    tangent_out = jnp.tanh(beta * x) * x_dot\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "def lagr_tanh(x, beta=1.0):\n",
    "    return _lagr_tanh(x, beta).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterized Lagrangians\n",
    "\n",
    "It is beneficial to consider lagrangians as modules with their own learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import treex as tx\n",
    "from dataclasses import dataclass\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LIdentity(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the identity function\"\"\"\n",
    "    def __init__(self): pass\n",
    "    def __call__(self, x):\n",
    "        return lagr_identity(x)\n",
    "\n",
    "class LRepu(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the rectified polynomial unit of specified degree `n`\"\"\"\n",
    "    n: float = 2\n",
    "    \n",
    "    # Need a default for `n` to work with layer creation\n",
    "    def __init__(self,\n",
    "                 n=2.): # The degree of the RePU. By default, set to the ReLU configuration\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return lagr_repu(x, self.n)\n",
    "    \n",
    "class LRelu(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the rectified linear unit\"\"\"\n",
    "    def __init__(self): pass\n",
    "    def __call__(self, x):\n",
    "        return lagr_relu(x)\n",
    "    \n",
    "class LSigmoid(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the sigmoid\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    - beta\n",
    "    \"\"\"\n",
    "    beta: Union[float, jnp.ndarray] = tx.Parameter.node(default=1.0)\n",
    "    scale: float = tx.Parameter.node(default=1.0)\n",
    "    min_beta: float = 1e-6\n",
    "    \n",
    "    def __init__(self, \n",
    "                 beta=1., # Inverse temperature\n",
    "                 scale=1., # Amount to stretch the sigmoid.\n",
    "                 min_beta=1e-6): # Minimal accepted value of beta. For energy dynamics, it is important that beta be positive.\n",
    "        self.beta = beta\n",
    "        self.scale = scale\n",
    "        self.min_beta = min_beta\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return lagr_simoid(x, beta=jnp.clip(self.beta, self.min_beta), scale=self.scale)\n",
    "    \n",
    "class LSoftmax(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the softmax\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    - beta\n",
    "    \"\"\"\n",
    "    beta: Union[float, jnp.ndarray] = tx.Parameter.node(default=1.0)\n",
    "    axis: int = -1\n",
    "    min_beta: float = 1e-6\n",
    "\n",
    "    def __init__(self, \n",
    "         beta=1., # Inverse temperature\n",
    "         axis=-1, # Axis over which to apply the softmax\n",
    "         min_beta=1e-6): # Minimal accepted value of beta. For energy dynamics, it is important that beta be positive.\n",
    "        self.beta = beta\n",
    "        self.axis = axis\n",
    "        self.min_beta = min_beta\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return lagr_softmax(x, beta=jnp.clip(self.beta, self.min_beta), axis=self.axis)\n",
    "    \n",
    "\n",
    "class LExp(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the exponential function\n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    - beta\n",
    "    \"\"\"\n",
    "    beta: Union[float, jnp.ndarray] = tx.Parameter.node(default=1.0)\n",
    "    min_beta: float = 1e-6\n",
    "\n",
    "    def __init__(self, \n",
    "                 beta=1., # Inverse temperature, for the sharpness of the exponent\n",
    "                 min_beta=1e-6): # Minimal accepted value of beta. For energy dynamics, it is important that beta be positive.\n",
    "        self.beta = beta\n",
    "        self.min_beta = min_beta\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return lagr_exp(x, beta=jnp.clip(self.beta, self.min_beta))\n",
    "    \n",
    "class LTanh(tx.Module):\n",
    "    \"\"\"Lagrangian whose activation function is the tanh\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    - beta\n",
    "    \"\"\"\n",
    "    beta: Union[float, jnp.ndarray] = tx.Parameter.node(default=1.0)\n",
    "    min_beta: float = 1e-6\n",
    "\n",
    "    def __init__(self, \n",
    "                 beta=1., # Inverse temperature, for the sharpness of the exponent\n",
    "                 min_beta=1e-6): # Minimal accepted value of beta. For energy dynamics, it is important that beta be positive.\n",
    "        self.beta = beta\n",
    "        self.min_beta = min_beta\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return lagr_tanh(x, beta=jnp.clip(self.beta,self.min_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdc5f107602d302d3282ec40de923ef553a5ac4d122eec8045c417a16238788c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
