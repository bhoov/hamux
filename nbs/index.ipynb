{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "bibliography: references.bib\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAMUX\n",
    "\n",
    "> A New Class of  Deep Learning Library Built around **ENERGY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/header.png\" alt=\"HAMUX Logo\" width=\"400\"/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|echo: false\n",
    "display(HTML(\"\"\"\n",
    "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/header.png\" alt=\"HAMUX Logo\" width=\"400\"/>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part proof-of-concept, part functional prototype, HAMUX is designed to bridge modern AI architectures and biologically plausible Hopfield Networks.\n",
    "\n",
    "\n",
    "**HAMUX**: A **H**ierarchical **A**ssociative **M**emory **U**ser e**X**perience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"alert alert-info\">\n",
       "    üöß <strong>API is in rapid development</strong>. Remember to specify the version when building off of HAMUX.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|echo: false\n",
    "display(HTML(\"\"\"\n",
    "<div class=\"alert alert-info\">\n",
    "    üöß <strong>API is in rapid development</strong>. Remember to specify the version when building off of HAMUX.\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Universal Abstraction for Hopfield Networks\n",
    "\n",
    "HAMUX fully captures the the energy fundamentals of Hopfield Networks and enables anyone to:\n",
    "\n",
    "- üß† Build **DEEP** Hopfield nets\n",
    "\n",
    "- üß± With modular **ENERGY** components\n",
    "\n",
    "- üèÜ That resemble modern DL operations\n",
    "\n",
    "**Every** architecture built using HAMUX is a *dynamical system* guaranteed to have a *tractable energy* function that *converges* to a fixed point. Our deep [Hierarchical Associative Memories](https://arxiv.org/abs/2107.06446) (HAMs) have several additional advantages over traditional [Hopfield Networks](https://en.wikipedia.org/wiki/Hopfield_network) (HNs):\n",
    "\n",
    "| Hopfield Networks (HNs) | Hierarchical Associative Memories (HAMs) |\n",
    "|--------|------|\n",
    "|HNs are only **two layers** systems | HAMs connect **any number** of layers|\n",
    "|HNs model only **linear relationships** between layers | HAMs model **any differentiable operation** (e.g., convolutions, pooling, attention, $\\ldots$)| \n",
    "|HNs use only **pairwise synapses** | HAMs use **many-body synapses** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does HAMUX work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HAMUX** is a <a href=\"https://en.wikipedia.org/wiki/Hypergraph\" >hypergraph</a> of üåÄneurons connected via ü§ùsynapses, an abstraction sufficiently general to model the complexity of connections contained in the üß†. \n",
    "\n",
    "HAMUX defines two fundamental building blocks of energy: the **üåÄneuron layer** and the **ü§ùsynapse**, connected via a **hypergraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/fig1.png\" alt=\"HAMUX Overview\" width=\"700\"/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|echo: false\n",
    "display(HTML(\"\"\"\n",
    "<img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/fig1.png\" alt=\"HAMUX Overview\" width=\"700\"/>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄNeuron Layers\n",
    "\n",
    "Neuron layers are the recurrent unit of a HAM; that is, üåÄneurons keep a state that changes over time according to the dynamics of the system. These states always change to minimize the global energy function of the system.\n",
    "\n",
    "For those of us familiar with traditional Deep Learning architectures, we are familiar with nonlinear activation functions like the `ReLU` and `SoftMax`. A neuron layer in HAMUX is exactly that: a nonlinear activation function defined on some neuron. However, we need to express the activation function as a convex **Lagrangian function** $\\mathcal{L}$ that is the integral of the desired non-linearity such that the **derivative of the Lagrangian function** $\\nabla \\mathcal{L}$ is our desired non-linearity. E.g., consider the ReLU:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(x) &:= \\frac{1}{2} (\\max(x, 0))^2\\\\\n",
    "\\nabla \\mathcal{L} &= \\max(x, 0) = \\mathrm{relu}(x)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "We need to define our activation layer in terms of the *Lagrangian* of the ReLU instead of the ReLU itself. Extending this constraint to other nonlinearities makes it possible to define the scalar energy for any neuron in a HAM. It turns out that many activation functions used in today's Deep Learning landscape are expressible as a Lagrangian. HAMUX is \"batteries-included\" for many common activation functions including `relu`s, `softmax`es, `sigmoid`s, `LayerNorm`s, etc. See our [documentation on Lagrangians](https://bhoov.github.io/hamux/lagrangians.html) for examples on how to implement efficient activation functions from Lagrangians in JAX. We show how to turn Lagrangians into usable energy building blocks in our [documentation on neuron layers](https://bhoov.github.io/hamux/layers.html).\n",
    "\n",
    "\n",
    "### ü§ùSynapses\n",
    "\n",
    "A ü§ùsynapse ONLY sees activations of connected üåÄneuron layers. Its one job: report HIGH ‚ö°Ô∏èenergy if the connected activations are dissimilar and LOW ‚ö°Ô∏èenergy when they are aligned. Synapses can resemble convolutions, dense multiplications, even attention‚Ä¶ Take a look at our [documentation on synapses](https://bhoov.github.io/hamux/synapses.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"alert alert-info\">\n",
       "    üö® <strong>Point of confusion</strong>: modern AI frameworks have <code>AttentionLayer</code>s and <code>ConvolutionalLayer</code>s. In HAMUX, these would be more appropriately called <code>AttentionSynapse</code>s and <code>ConvolutionalSynapse</code>s.\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|echo: false\n",
    "display(HTML(\"\"\"\n",
    "<div class=\"alert alert-info\">\n",
    "    üö® <strong>Point of confusion</strong>: modern AI frameworks have <code>AttentionLayer</code>s and <code>ConvolutionalLayer</code>s. In HAMUX, these would be more appropriately called <code>AttentionSynapse</code>s and <code>ConvolutionalSynapse</code>s.\n",
    "</div>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "**From pip**:\n",
    "\n",
    "```\n",
    "pip install hamux\n",
    "```\n",
    "\n",
    "If you are using accelerators beyond the CPU you will need to additionally install the corresponding `jax` and `jaxlib` versions following [their documentation](https://github.com/google/jax#installation). E.g.,\n",
    "\n",
    "```\n",
    "pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "```\n",
    "\n",
    "\n",
    "**From source**:\n",
    "\n",
    "After cloning:\n",
    "```\n",
    "cd hamux\n",
    "conda env create -f environment.yml\n",
    "conda activate hamux\n",
    "pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html # If using GPU accelerator\n",
    "pip install -e .\n",
    "pip install -r requirements-dev.txt  # To run the examples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hamux as hmx\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.tree_util as jtu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a simple 4 layer HAM architecture using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 01:25:17.022577: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "#|output:false\n",
    "layers = [\n",
    "    hmx.TanhLayer((32,32,3)), # e.g., CIFAR Images\n",
    "    hmx.SigmoidLayer((11,11,1000)), # CIFAR patches\n",
    "    hmx.SoftmaxLayer((10,)), # CIFAR Labels\n",
    "    hmx.SoftmaxLayer((1000,)), # Hidden Memory Layer\n",
    "]\n",
    "\n",
    "synapses = [\n",
    "    hmx.ConvSynapse((3,3), strides=3),\n",
    "    hmx.DenseSynapse(),\n",
    "    hmx.DenseSynapse(),\n",
    "]\n",
    "\n",
    "connections = [\n",
    "    ([0,1], 0),\n",
    "    ([1,3], 1),\n",
    "    ([2,3], 2),\n",
    "]\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "param_key, state_key, rng = jax.random.split(rng, 3)\n",
    "states, ham = hmx.HAM(layers, synapses, connections).init_states_and_params(param_key, state_key=state_key);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not specify any output channel shapes in the synapses. The desired output shape is computed from the layers connected to each synapse during `hmx.HAM.init_states_and_params`.\n",
    "\n",
    "We have two fundamental objects: `states` and `ham`. The `ham` object contains the connectivity structure of the HAM (e.g., layer+synapse+hypergraph information) alongside the **parameters** of the network. The `states` object is a list of length `nlayers` where each item is a tensor representing the neuron states of the corresponding layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(states) == ham.n_layers\n",
    "assert all([state.shape == layer.shape for state, layer in zip(states, ham.layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make it easy to run the dynamics of any HAM. Every `forward` function is defined external to the memory and can be modified to extract different memories from different layers, as desired. The general steps for any forward function are:\n",
    "\n",
    "1. Initialize the dynamic states\n",
    "2. Inject an initial state into the system\n",
    "3. Run dynamics, calculating energy gradient at every point in time.\n",
    "4. Return the layer state/activation of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd(model, x, depth=15, dt=0.1):\n",
    "    \"\"\"Assuming a trained HAM, run association with the HAM on batched inputs `x`\"\"\"\n",
    "    # 1. Initialize model states at t=0. Account for batch size\n",
    "    xs = model.init_states(x.shape[0])\n",
    "    \n",
    "    # Inject initial state\n",
    "    xs[0] = x \n",
    "\n",
    "    energies = []\n",
    "    for i in range(depth):\n",
    "        energies.append(model.venergy(xs)) # If desired, observe the energy\n",
    "        dEdg = model.vdEdg(xs)  # Calculate the gradients\n",
    "        xs = jtu.tree_map(lambda x, stepsize, grad: x - stepsize * grad, xs, model.alphas(dt), dEdg)\n",
    "\n",
    "    \n",
    "    # Return probabilities of our label layer\n",
    "    probs = model.layers[-2].activation(xs[-2])\n",
    "    return jnp.stack(energies), probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=3\n",
    "x = jax.random.normal(jax.random.PRNGKey(2), (batch_size, 32,32,3))\n",
    "energies, probs = fwd(ham, x, depth=20, dt=0.3)\n",
    "print(probs.shape) # batchsize, nclasses\n",
    "assert jnp.allclose(probs.sum(-1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(energies)\n",
    "ax.set_title(\"Convergence of HAM Energy over time\")\n",
    "ax.set_xlabel(\"Timesteps\")\n",
    "ax.set_ylabel(\"Energy\");\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Energy Function vs the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use JAX's autograd to descend the energy function of our system AND the loss function of our task. The derivative of the energy is always taken wrt to our *states*; the derivative of the loss function is always taken wrt our *parameters*. During training, we change our parameters to optimize the *Loss Function*. During inference, we assume that parameters are constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd for Descending Energy**\n",
    "\n",
    "Every `HAM` defines the energy function for our system, which is everything we need to compute memories of the system. Naively, we can calculate $\\nabla_x E$: the derivative of the energy function wrt the *states* of each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize = 0.01\n",
    "fscore_naive = jax.grad(ham.energy)\n",
    "next_states = jax.tree_util.tree_map(lambda state, score: state - stepsize, states, fscore_naive(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it turns out we improve the efficiency of our network if we instead take $\\nabla_g E$: the derivative of the energy wrt. the *activations* instead of the *states*. They have the same local minima, even though the trajectory to get there is different. Some nice terms cancel, and we get:\n",
    "\n",
    "$$\\nabla_g E_\\text{HAM} = x + \\nabla_g E_\\text{synapse}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsize = 0.01\n",
    "def fscore_smart(xs):\n",
    "    gs = ham.activations(xs)\n",
    "    return jax.tree_util.tree_map(lambda x, nabla_g_Esyn: x + nabla_g_Esyn, xs, jax.grad(ham.synapse_energy)(gs))\n",
    "\n",
    "next_states = jax.tree_util.tree_map(lambda state, score: state - stepsize, states, fscore_smart(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "\n",
    "Work is a collaboration between the [MIT-IBM Watson AI Lab](https://mitibmwatsonailab.mit.edu/) and the [PoloClub](https://poloclub.github.io/) @ GA Tech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
