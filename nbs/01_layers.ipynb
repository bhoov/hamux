{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Layers\n",
    "\n",
    "> Turning Lagrangians into building blocks\n",
    "\n",
    "Fundamentally, a neuron layer is nothing more than a lagrangian function on top of data. This means that, in addition to a lagrangian, a neuron layer has:\n",
    "\n",
    "- A `shape`\n",
    "- A time constant `tau`\n",
    "- A `bias` (optional) that we can view as the activation threshold of a neuron layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import *\n",
    "import treex as tx\n",
    "from abc import ABC, abstractmethod\n",
    "from flax import linen as nn\n",
    "from hamux.lagrangians import *\n",
    "import functools as ft\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Layer(tx.Module):\n",
    "    \"\"\"The energy building block of any activation in our network that we want to hold state over time\"\"\"\n",
    "    lagrangian: tx.Module \n",
    "    shape: Tuple\n",
    "    tau: float\n",
    "    use_bias: bool\n",
    "    bias: jnp.ndarray = tx.Parameter.node(default=None)\n",
    "\n",
    "    def __init__(self, \n",
    "                 lagrangian:tx.Module, # Describes the non-linearity\n",
    "                 shape:Tuple[int], # Number and shape of neuron assembly\n",
    "                 tau:float=1.0, # Time constant\n",
    "                 use_bias:bool=False, # Add bias?\n",
    "                 **kwargs): # Arguments passed to initialize the lagrangian\n",
    "        self.lagrangian = lagrangian(**kwargs)\n",
    "        self.shape = shape\n",
    "        assert tau > 0.0, \"Tau must be positive and non-zero\"\n",
    "        self.tau = tau\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def activation(self, x):\n",
    "        \"\"\"Alias for `self.g`\"\"\"\n",
    "        return self.g(x)\n",
    "\n",
    "    def energy(self, x):\n",
    "        \"\"\"The predefined energy of a layer, defined for any lagrangian\"\"\"\n",
    "        if self.initializing():\n",
    "            if self.use_bias:\n",
    "                self.bias = nn.initializers.normal(0.02)(tx.next_key(), self.shape)\n",
    "        x2 = x - self.bias if self.use_bias else x # Is this an issue?\n",
    "\n",
    "        # When jitted, this is no slower than the optimized `@` vector multiplication\n",
    "        return jnp.multiply(self.g(x), x2).sum() - self.lagrangian(x2)\n",
    "\n",
    "    def g(self, x):\n",
    "        \"\"\"The derivative of the lagrangian is our activation or Gain function `g`. \n",
    "        \n",
    "        Defined to operate over input states `x` of shape `self.shape`\n",
    "        \"\"\"\n",
    "        if self.initializing():\n",
    "            if self.use_bias:\n",
    "                self.bias = nn.initializers.normal(0.02)(tx.next_key(), self.shape)\n",
    "        x2 = x - self.bias if self.use_bias else x\n",
    "        return jax.grad(self.lagrangian)(x2)\n",
    "\n",
    "    def init_state(self, \n",
    "                   bs: int = None, # Batch size\n",
    "                   rng=None): # If given, initialize states from a normal distribution with this key\n",
    "        \"\"\"Initialize the states of this layer, with correct shape.\n",
    "        \n",
    "        If `bs` is provided, return tensor of shape (bs, *self.shape), otherwise return self.shape\n",
    "        By default, initialize layer state to all 0.\n",
    "        \"\"\"\n",
    "        layer_shape = self.shape if bs is None else (bs, *self.shape)\n",
    "        if rng is not None:\n",
    "            return jax.random.normal(rng, layer_shape)\n",
    "        return jnp.zeros(layer_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nice to package commonly used lagrangians as their own kind of layer, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def MakeLayer(lagrangian_factory):\n",
    "    \"\"\"Hack to make it easy to create new layers from `Layer` utility class.\n",
    "    \n",
    "    `delegates` modifies the signature for all Layers. We want a different signature for each type of layer.\n",
    "\n",
    "    So we redefine a local version of layer and delegate that for type inference.\n",
    "    \"\"\"\n",
    "    global Layer\n",
    "\n",
    "    @delegates(lagrangian_factory, keep=True)\n",
    "    class Layer(Layer):\n",
    "        __doc__ = Layer.__doc__\n",
    "        \n",
    "    out = partialler(Layer, lagrangian_factory)\n",
    "    out.__doc__ = Layer.__doc__\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Some reason, docstrings are not showing the new kwargs, and the docs for these are broken. \n",
    "IdentityLayer = MakeLayer(LIdentity)\n",
    "RepuLayer = MakeLayer(LRepu)\n",
    "ReluLayer = MakeLayer(LRelu)\n",
    "SoftmaxLayer = MakeLayer(LSoftmax)\n",
    "SigmoidLayer = MakeLayer(LSigmoid)\n",
    "TanhLayer = MakeLayer(LTanh)\n",
    "ExpLayer = MakeLayer(LExp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### LSigmoid'>)\n",
       "\n",
       ">      LSigmoid'>) (shape:Tuple[int], tau:float=1.0, use_bias:bool=False,\n",
       ">                   beta=1.0, scale=1.0, min_beta=1e-06, **kwargs)\n",
       "\n",
       "The energy building block of any activation in our network that we want to hold state over time"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### LSigmoid'>)\n",
       "\n",
       ">      LSigmoid'>) (shape:Tuple[int], tau:float=1.0, use_bias:bool=False,\n",
       ">                   beta=1.0, scale=1.0, min_beta=1e-06, **kwargs)\n",
       "\n",
       "The energy building block of any activation in our network that we want to hold state over time"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SigmoidLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our utility that we use to create these \"convenience layers\" is a bit hacky, but it works by injecting the lagrangian and the expected arguments for the lagrangian into our `Layer` utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bhoov/hamux/blob/main/hamux/layers.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MakeLayer\n",
       "\n",
       ">      MakeLayer (lagrangian_factory)\n",
       "\n",
       "Hack to make it easy to create new layers from `Layer` utility class.\n",
       "\n",
       "`delegates` modifies the signature for all Layers. We want a different signature for each type of layer.\n",
       "\n",
       "So we redefine a local version of layer and delegate that for type inference."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bhoov/hamux/blob/main/hamux/layers.py#L80){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MakeLayer\n",
       "\n",
       ">      MakeLayer (lagrangian_factory)\n",
       "\n",
       "Hack to make it easy to create new layers from `Layer` utility class.\n",
       "\n",
       "`delegates` modifies the signature for all Layers. We want a different signature for each type of layer.\n",
       "\n",
       "So we redefine a local version of layer and delegate that for type inference."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MakeLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this hack, we lose the ability to inspect docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdc5f107602d302d3282ec40de923ef553a5ac4d122eec8045c417a16238788c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
