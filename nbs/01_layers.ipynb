{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Layers\n",
    "\n",
    "> Turning Lagrangians into building blocks for our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, a neuron layer is nothing more than a lagrangian function on top of data. This Lagrangian completely defines both an **energy** and an **activation** for the layer. In practice, we additionally specify the following in addition to the Lagrangian:\n",
    "\n",
    "- A `shape`\n",
    "- A time constant `tau`\n",
    "- A `bias` (optional) that we can view as the activation threshold of a neuron layer\n",
    "\n",
    "Our neuron layers are extensions of the neuron layer as it is commonly incorporated in feedforward architectures. The primary difference is that our neuron layers *evolve their state $x$ over time* and have a *bounded energy function* on their states. The energy function of our neuron is completely defined by its Lagrangian $\\mathcal{L}$\n",
    "\n",
    "$$E_\\text{layer} = \\sum\\limits_i x_i g_i - \\mathcal{L}(x)$$\n",
    "\n",
    "where $g = \\frac{\\partial \\mathcal{L}}{\\partial x}$ is also called the \"activations\". The first component is a summation of the elementwise multiplication between $x$ and $g$. The second term is the Lagrangian. This energy function is the direct consequence of the Legendre transform on the Lagrangian.\n",
    "\n",
    "We can view a neuron layer of shape `(D,)` as a collection of $D$ neurons holding scalar data. Convolutional networks frequently have activations defined atop images or image patches of shape `(D,H,W)`. We can view layers of this shape as a collection of $D$ neurons each of shape $(H,W)$. Lagrangians that reduce over a particular dimension (e.g., the `softmax`) will always reduce over the neuron dimension $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import *\n",
    "import treex as tx\n",
    "from abc import ABC, abstractmethod\n",
    "from flax import linen as nn\n",
    "from hamux.lagrangians import *\n",
    "import functools as ft\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Layer(tx.Module):\n",
    "    \"\"\"The energy building block of any activation in our network that we want to hold state over time\"\"\"\n",
    "    lagrangian: tx.Module \n",
    "    shape: Tuple\n",
    "    tau: float\n",
    "    use_bias: bool\n",
    "    bias: jnp.ndarray = tx.Parameter.node(default=None)\n",
    "\n",
    "    def __init__(self, \n",
    "                 lagrangian:tx.Module, # Describes the non-linearity\n",
    "                 shape:Tuple[int], # Number and shape of neuron assembly\n",
    "                 tau:float=1.0, # Time constant\n",
    "                 use_bias:bool=False, # Add bias?\n",
    "                 **kwargs): # Arguments passed to initialize the lagrangian\n",
    "        self.lagrangian = lagrangian(**kwargs)\n",
    "        self.shape = shape\n",
    "        assert tau > 0.0, \"Tau must be positive and non-zero\"\n",
    "        self.tau = tau\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def energy(self, x):\n",
    "        \"\"\"The predefined energy of a layer, defined for any lagrangian\"\"\"\n",
    "        if self.initializing():\n",
    "            if self.use_bias:\n",
    "                self.bias = nn.initializers.normal(0.02)(tx.next_key(), self.shape)\n",
    "        x2 = x - self.bias if self.use_bias else x # Is this an issue?\n",
    "\n",
    "        # When jitted, this is no slower than the optimized `@` vector multiplication\n",
    "        return jnp.multiply(self.g(x), x2).sum() - self.lagrangian(x2)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Alias for `self.energy`. Helps simplify treex's `.init` method\"\"\"\n",
    "        return self.energy(x)\n",
    "            \n",
    "    def activation(self, x):\n",
    "        \"\"\"The derivative of the lagrangian is our activation or Gain function `g`. \n",
    "        \n",
    "        Defined to operate over input states `x` of shape `self.shape`\n",
    "        \"\"\"\n",
    "        if self.initializing():\n",
    "            if self.use_bias:\n",
    "                self.bias = nn.initializers.normal(0.02)(tx.next_key(), self.shape)\n",
    "        x2 = x - self.bias if self.use_bias else x\n",
    "        return jax.grad(self.lagrangian)(x2)\n",
    "    \n",
    "    def g(self, x):\n",
    "        \"\"\"Alias for `self.activation`\"\"\"\n",
    "        return self.activation(x)\n",
    "\n",
    "    def init_state(self, \n",
    "                   bs: int = None, # Batch size\n",
    "                   rng=None): # If given, initialize states from a normal distribution with this key\n",
    "        \"\"\"Initialize the states of this layer, with correct shape.\n",
    "        \n",
    "        If `bs` is provided, return tensor of shape (bs, *self.shape), otherwise return self.shape\n",
    "        By default, initialize layer state to all 0.\n",
    "        \"\"\"\n",
    "        layer_shape = self.shape if bs is None else (bs, *self.shape)\n",
    "        if rng is not None:\n",
    "            return jax.random.normal(rng, layer_shape)\n",
    "        return jnp.zeros(layer_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Layer.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Layer.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Layer.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Layer.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Layer.init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is nice to package commonly used lagrangians into their own kind of layers, e.g., `IdentityLayer`s or `SoftmaxLayer`s. We create a helper function to do that in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "def MakeLayer(lagrangian_factory):\n",
    "    \"\"\"Hack to make it easy to create new layers from `Layer` utility class.\n",
    "    \n",
    "    `delegates` modifies the signature for all Layers. We want a different signature for each type of layer.\n",
    "\n",
    "    So we redefine a local version of layer and delegate that for type inference.\n",
    "    \"\"\"\n",
    "    global Layer\n",
    "\n",
    "    @delegates(lagrangian_factory, keep=True)\n",
    "    class Layer(Layer):\n",
    "        __doc__ = Layer.__doc__\n",
    "        \n",
    "    out = partialler(Layer, lagrangian_factory)\n",
    "    out.__doc__ = Layer.__doc__\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Some reason, docstrings are not showing the new kwargs, and the docs for these are broken. \n",
    "IdentityLayer = MakeLayer(LIdentity)\n",
    "RepuLayer = MakeLayer(LRepu)\n",
    "ReluLayer = MakeLayer(LRelu)\n",
    "SoftmaxLayer = MakeLayer(LSoftmax)\n",
    "SigmoidLayer = MakeLayer(LSigmoid)\n",
    "TanhLayer = MakeLayer(LTanh)\n",
    "ExpLayer = MakeLayer(LExp)\n",
    "RexpLayer = MakeLayer(LRexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SigmoidLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our utility that we use to create these \"convenience layers\" is a bit hacky, but it works by injecting the lagrangian and the expected arguments for the lagrangian into our `Layer` utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MakeLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this hack, we lose the ability to inspect docstrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of all, every neuron layer has an **energy**. This is defined by the following equation:\n",
    "\n",
    "$$E_\\text{layer} = \\sum\\limits_i x_i g_i - \\mathcal{L}(x)$$\n",
    "\n",
    "The first component is a summation of the elementwise multiplication between $x$ and $g$. The second term is the lagrangian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a layer with state $x$ and initial state $x_0$. Let us then evolve $x$ over time by simply descending the energy of the neuron layer. Note that this neuron layer is not connected to anything, but we still expect it to reach a fixed point where the energy of the layer reaches a fixed point.\n",
    "\n",
    "$$\\tau\\frac{dx}{dt} = -\\frac{dE_\\text{layer}}{dx}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "shape = (5,); key = jax.random.PRNGKey(0); k1, k2, key = jax.random.split(key,3)\n",
    "layer = ExpLayer(shape,beta=1.).init(k1, jnp.ones(shape))\n",
    "x0 = layer.init_state(rng=k2)\n",
    "\n",
    "@ft.partial(jax.jit, static_argnames=(\"alpha\",))\n",
    "def next_x(layer:Layer, # Neuron layer\n",
    "           x:jnp.ndarray, # Current state\n",
    "           alpha:float): # Step size\n",
    "    dEdx = jax.value_and_grad(layer.energy)\n",
    "    E, dx = dEdx(x)\n",
    "    next_x = x -alpha * dx\n",
    "    return E, next_x\n",
    "\n",
    "x = x0\n",
    "Es = []\n",
    "for i in range(100):\n",
    "    E, x = next_x(layer, x, 0.1)\n",
    "    Es.append(E)\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.stack(Es))\n",
    "ax.set_title(\"Energy of a disconnected neuron layer over time\")\n",
    "ax.set_ylabel(\"Energy\")\n",
    "ax.set_xlabel(\"Timesteps\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy is bounded for any initial neuron state $x$ (though at some point the values are numerically too large for the exponential we are using as the activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "x = 100*x0\n",
    "Es = []\n",
    "for i in range(100):\n",
    "    E, x = next_x(layer, x, 5e-8)\n",
    "    Es.append(E)\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(np.stack(Es))\n",
    "ax.set_title(\"Large $x_0$, small $dt$\")\n",
    "ax.set_ylabel(\"Energy\")\n",
    "ax.set_xlabel(\"Timesteps\")\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
