{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Hyper)Synapses\n",
    "\n",
    "> Using Modern Deep Learning operations with Energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#|echo:false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m HTML(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m<figure>\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    <img src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/bhoov/hamux/main/assets/HypersynapseEnergy.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m alt=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHopfield Synapse Description\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m width=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/>\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    <figcaption style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor:#999;\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>The generalized energy of any hypersynapse as used in the Hopfield paradigm.</figcaption>\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m</figure>\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "#|echo:false\n",
    "HTML(\"\"\"\n",
    "<figure>\n",
    "    <img src=\"https://raw.githubusercontent.com/bhoov/hamux/main/assets/HypersynapseEnergy.png\" alt=\"Hopfield Synapse Description\" width=\"500\"/>\n",
    "    <figcaption style=\"color:#999;\">The generalized energy of any hypersynapse.</figcaption>\n",
    "</figure>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypersynapses** are the only way that neuron layers can communicate with each other. This generalization of the pairwise synapse from the Hopfield paradigm is now more general than ever before:\n",
    "\n",
    "\n",
    "| Before | HAMUX |\n",
    "|--------|------|\n",
    "|Synapses connect only one neuron layer to another | Hypersynapses can connect **arbitrary numbers** of layers |\n",
    "|Synapses are simple matrix multiplications | Hypersynapses can be almost **any operation**, e.g., convolutions, pooling, attention, $\\ldots$| \n",
    "|Synapses are shallow | Hypersynapses can be **deep**! E.g., a sequence of convolutions, pooling, and activation functions |\n",
    "\n",
    "\n",
    "At its core, a hypersynapse's energy is completely defined by its *alignment function* $\\mathcal{F}$ that converts any number of layer activations $(g^1, g^2, \\ldots)$ into a scalar describing its alignment:\n",
    "\n",
    "$$\n",
    " E_{\\text{synapse}} = -\\mathcal{F},\\ \\ \\ \\ \\text{where}\\ \\ \\ \\  \\mathcal{F} (g^1, g^2, \\ldots) \\mapsto \\mathbb{R}.\n",
    " $$\n",
    " \n",
    "The hypersynapse's energy is typically HIGH when all connected layers are \"incongruous\" and LOW when all connected layers are \"aligned\" as defined by its operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import *\n",
    "import treex as tx\n",
    "from abc import ABC, abstractmethod\n",
    "from flax import linen as nn\n",
    "from hamux.lagrangians import *\n",
    "import functools as ft\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.utils import *\n",
    "from fastcore.basics import *\n",
    "from string import ascii_letters\n",
    "from flax.linen.pooling import max_pool, avg_pool\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Synapse`s conform to the following simple API. Just define a `__call__` function to describe the scalar alignment of different activations. Energy is calculated for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Synapse(tx.Module, ABC):\n",
    "    \"\"\"The simple interface class for any synapse. Define an alignment function through `__call__` that returns a scalar.\n",
    "\n",
    "    The energy is simply the negative of this function.\n",
    "    \"\"\"\n",
    "\n",
    "    def energy(self, *gs):\n",
    "        return -self(*gs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *gs):\n",
    "        \"\"\"The alignment function of a synapse\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bhoov/hamux/blob/main/hamux/synapses.py#L31){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Synapse.energy\n",
       "\n",
       ">      Synapse.energy (*gs)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bhoov/hamux/blob/main/hamux/synapses.py#L31){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Synapse.energy\n",
       "\n",
       ">      Synapse.energy (*gs)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(Synapse.energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Synapse\n",
    "\n",
    "The simplest of synapses is a dense alignment synapse. In feedforward networks, dense operations take an input and return an output. In HAMUX, dense operations align the activations $g^1 \\in \\mathbb{R}^{D_1}$ and $g^2 \\in \\mathbb{R}^{D_2}$ as follows:\n",
    "\n",
    "$$\\mathcal{F}_\\text{dense} = g^1_i W_{ij} g^2_j$$\n",
    "\n",
    "And would be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SimpleDenseSynapse(Synapse):\n",
    "    \"\"\"The simplest of dense synapses that connects two layers (with vectorized activations) together\"\"\"\n",
    "    W: jnp.ndarray = tx.Parameter.node() # treex's preferred way of declaring an attribute as a parameter\n",
    "    def __call__(self, g1, g2):\n",
    "        if self.initializing():\n",
    "            self.W = nn.initializers.normal(0.02)(tx.next_key(), g1.shape + g2.shape)\n",
    "        return g1 @ self.W @ g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-02 01:13:30.993496: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(-0.09093504, dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = jnp.ones(4, dtype=jnp.float32); g2 = jnp.ones(5, dtype=jnp.float32)\n",
    "syn = SimpleDenseSynapse().init(jax.random.PRNGKey(0), (g1, g2))\n",
    "syn(g1, g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building HAMs in practice, we typically want to follow this pattern: subclass the minimal `Synapse` class and overwrite the `Synapse.__call__` method with our desired alignment function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend this simple concept into a more robust synapse that can linearly connect $>2$ layers and optionally flattens layer activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DenseSynapse(Synapse):\n",
    "    \"\"\"A dense synapse that aligns the representations of any number of `gs`.\n",
    "\n",
    "    The one learnable parameter `W` is a tensor with a dimension for each connected layer.\n",
    "    In the case of 2 layers, this is the traditional learnable matrix synapse.\n",
    "    In cases `N>2` layers this is a new kind of layer where the learnable parameter is an N dimensional tensor.\n",
    "\n",
    "    By default, this will flatten all inputs as needed to treat all activations as vectors. \n",
    "    \n",
    "    The number of layers we can align with this synapse is capped at the number of ranks that JAX stores (<255),\n",
    "    but you'll probably run out of memory first..\n",
    "    \"\"\"\n",
    "\n",
    "    W: jnp.ndarray = tx.Parameter.node()\n",
    "    stdinit: float = 0.02\n",
    "    flatten_args: bool = True\n",
    "\n",
    "    def __init__(self, stdinit: float = 0.02, flatten_args=True):\n",
    "        self.stdinit = stdinit\n",
    "        self.flatten_args = flatten_args\n",
    "\n",
    "    def __call__(self, *gs):\n",
    "        if self.initializing():\n",
    "            ndims_total = jnp.sum(jnp.array([len(g.shape) for g in gs]))\n",
    "            assert (\n",
    "                ndims_total <= 52\n",
    "            ), f\"We are limited to english ASCII letters. We cannot connect more than 52 dimensions. Got {ndims_total} total dimensions.\"\n",
    "            if self.flatten_args:\n",
    "                gshapes = tuple([g_.size for g_ in gs])\n",
    "            else:\n",
    "                gshapes = tuple([g_.shape[-1] for g_ in gs])\n",
    "            self.W = nn.initializers.normal(self.stdinit)(tx.next_key(), gshapes)\n",
    "        if self.flatten_args:\n",
    "            gs = [g_.ravel() for g_ in gs]\n",
    "            abcs = ascii_letters[: len(gs)]\n",
    "            einsum_arg = \",\".join([abcs, \",\".join(abcs)]) + \"->\"\n",
    "            return jnp.einsum(einsum_arg, self.W, *gs)\n",
    "        else:\n",
    "            # Design the einsum to take letter positions corresponding to the\n",
    "            Wabcs = \"\"\n",
    "            gabcs = []\n",
    "            i = 0\n",
    "            for g in gs:\n",
    "                ndims = len(g.shape)\n",
    "                Wabcs += ascii_letters[(i - 1) + ndims]\n",
    "                gabcs.append(ascii_letters[i : i + ndims])\n",
    "                i = i + ndims\n",
    "            einsum_arg = \",\".join([Wabcs, \",\".join(gabcs)]) + \"->\"\n",
    "            return jnp.einsum(einsum_arg, self.W, *gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = jnp.ones(4, dtype=jnp.float32); g2 = jnp.ones(5, dtype=jnp.float32); g3 = jnp.ones(6, dtype=jnp.float32)\n",
    "syn = DenseSynapse().init(jax.random.PRNGKey(0), (g1, g2, g3))\n",
    "syn(g1, g2, g3)\n",
    "assert syn.W.shape == (4,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even implement a `DenseSynapse` with a hidden layer (Lagrangian) inside the alignment function. This is how we can implement layers that do not need to hold state through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DenseTensorSynapseWithHiddenLayer(Synapse):\n",
    "    \"\"\"A generalized DenseTensorSynapse that has a hidden lagrangian (non-linearity).\n",
    "\n",
    "    We can specify a Lagrangian non-linearity for the hidden neuron layer with tau=0 and shape `(nhid,)`.\n",
    "\n",
    "    The lagrangian can have its own learnable parameters, for example:\n",
    "\n",
    "    ```\n",
    "    from hamux.core.lagrangians import *\n",
    "\n",
    "    syn = DenseTerminusSynapse(20, hidden_lagrangian=LSoftmax(beta_init=0.2)).init(jax.random.PRNGKey(0), tuple(gs))\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    W: jnp.ndarray = tx.Parameter.node()\n",
    "    hidden_lagrangian: Optional[tx.Module]  # An already initialized lagrangian function\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nhid: int,\n",
    "        num_heads=1,\n",
    "        stdinit: float = 0.02,\n",
    "        hidden_lagrangian: tx.Module = LRelu(),\n",
    "    ):\n",
    "        self.nhid = nhid\n",
    "        self.stdinit = stdinit\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_lagrangian = hidden_lagrangian\n",
    "\n",
    "    def __call__(self, *gs):\n",
    "        if self.initializing():\n",
    "            assert (\n",
    "                len(gs) <= 52\n",
    "            ), \"We are limited to english ASCII letters. We cannot connect more than 50 layers if you include our hidden layer and number of heads\"\n",
    "            key = tx.next_key()\n",
    "            gshapes = (self.num_heads, self.nhid) + tuple([g_.size for g_ in gs])\n",
    "            self.W = nn.initializers.normal(self.stdinit)(key, gshapes)\n",
    "\n",
    "        gs = [g_.ravel() for g_ in gs]\n",
    "        abcs = ascii_letters[: len(gs)]\n",
    "        einsum_arg = (\n",
    "            \",\".join([\"YZ\" + abcs, \",\".join(abcs)]) + \"->YZ\"\n",
    "        )  # We call the nhid dimension \"Z\" and number of heads \"Y\"\n",
    "        x = jnp.einsum(einsum_arg, self.W, *gs)\n",
    "        return self.hidden_lagrangian(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DenseMatrixSynapseWithHiddenLayer(Synapse):\n",
    "    \"\"\"A modified DenseSynapse that has a hidden lagrangian (non-linearity).\n",
    "\n",
    "    We can specify a Lagrangian non-linearity for the hidden neuron layer with tau=0 and shape `(nhid,)`.\n",
    "\n",
    "    Unlike the DenseTensorSynapseWithHiddenLayer, treat layers as if they are concatenated on the same\n",
    "    visible layer dimension instead of giving each its own dimension of the tensor space.\n",
    "    \"\"\"\n",
    "\n",
    "    Ws: List[jnp.ndarray] = tx.Parameter.node()\n",
    "    hidden_lagrangian: Optional[tx.Module]  # An already initialized lagrangian function\n",
    "\n",
    "    def __init__(\n",
    "        self, nhid: int, stdinit: float = 0.02, hidden_lagrangian: tx.Module = LRelu(), do_ravel=True, do_norm=False,\n",
    "    ):\n",
    "        self.nhid = nhid\n",
    "        self.stdinit = stdinit\n",
    "        self.hidden_lagrangian = hidden_lagrangian\n",
    "        self.do_ravel = do_ravel\n",
    "        self.do_norm = do_norm\n",
    "\n",
    "    def __call__(self, *gs):\n",
    "        if self.initializing():\n",
    "            def initw(g):\n",
    "                if self.do_ravel:\n",
    "                    gsize = g.size\n",
    "                else:\n",
    "                    gsize = g.shape[-1]\n",
    "                return nn.initializers.normal(self.stdinit)(tx.next_key(), (gsize, self.nhid))\n",
    "            self.Ws = [initw(g_) for g_ in gs]\n",
    "\n",
    "        if self.do_ravel:\n",
    "            gs = [g_.ravel() for g_ in gs]\n",
    "        if self.do_norm:\n",
    "            Ws = [W / jnp.sqrt((W**2).sum(0,keepdims=True)) for W in self.Ws]\n",
    "        else:\n",
    "            Ws = self.Ws\n",
    "        hid_state = jnp.stack([g @ W for (W, g) in zip(Ws, gs)]).sum(0)\n",
    "        return self.hidden_lagrangian(hid_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvSynapse(Synapse):\n",
    "    \"\"\"A convolutional, binary synapse. Can automatically detect the number of output features from the 2 layers it connects\"\"\"\n",
    "\n",
    "    conv: tx.Conv\n",
    "    # Delegate arguments to conv EXCEPT the features_out, which we calculate from the output layer\n",
    "\n",
    "    @delegates(tx.Conv)\n",
    "    def __init__(self, kernel_size: Union[int, Iterable[int]], **kwargs):\n",
    "        # assert pool_type in [\"max\", \"avg\"]\n",
    "        self.kernel_size = kernel_size\n",
    "        conv_kwargs = {\"use_bias\": False}\n",
    "        conv_kwargs.update(kwargs)\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "\n",
    "    def example_output(self, g1, features_out=1):\n",
    "        \"\"\"Test the shape output of the convolutional layer. If unspecified, output features are 1\"\"\"\n",
    "        conv = tx.Conv(features_out, self.kernel_size, **self.conv_kwargs).init(\n",
    "            jax.random.PRNGKey(0), g1\n",
    "        )\n",
    "        return conv(g1)\n",
    "\n",
    "    def __call__(self, g1, g2):\n",
    "        \"\"\"The convolutional operation. g2 is assumed to be the \"output\" of the convolution\"\"\"\n",
    "        if self.initializing():\n",
    "            features_out = g2.shape[-1]\n",
    "            self.conv = tx.Conv(\n",
    "                features_out, self.kernel_size, **self.conv_kwargs\n",
    "            ).init(tx.next_key(), g1)\n",
    "        return jnp.multiply(self.conv(g1), g2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or contain pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvSynapseWithPool(Synapse):\n",
    "    \"\"\"A convolutional, binary synapse. Can automatically detect the number of output features from the 2 layers it connects\"\"\"\n",
    "\n",
    "    conv: tx.Conv\n",
    "    # Delegate arguments to conv EXCEPT the features_out, which we calculate from the output layer\n",
    "\n",
    "    @delegates(tx.Conv)\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: Union[int, Iterable[int]],\n",
    "        pool_window=(5, 5),\n",
    "        pool_stride=(2, 2),\n",
    "        pool_type=\"avg\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # assert pool_type in [\"max\", \"avg\"]\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = kwargs\n",
    "        self.pool_window = pool_window\n",
    "        self.pool_stride = pool_stride\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "        self.pooler = max_pool if self.pool_type == \"max\" else avg_pool\n",
    "        # self.conv = None\n",
    "\n",
    "    def example_output(self, g1, features_out=1):\n",
    "        \"\"\"Test the shape output of the convolutional layer. If unspecified, output features are 1\"\"\"\n",
    "        conv = tx.Conv(features_out, self.kernel_size, **self.conv_kwargs).init(\n",
    "            jax.random.PRNGKey(0), g1\n",
    "        )\n",
    "        output = self.pooler(conv(g1), self.pool_window, strides=self.pool_stride)\n",
    "        return output\n",
    "\n",
    "    def __call__(self, g1, g2):\n",
    "        \"\"\"The convolutional operation. g2 is assumed to be the \"output\" of the convolution\"\"\"\n",
    "        if self.initializing():\n",
    "            features_out = g2.shape[-1]\n",
    "            self.conv = tx.Conv(\n",
    "                features_out, self.kernel_size, **self.conv_kwargs\n",
    "            ).init(tx.next_key(), g1)\n",
    "        output = self.pooler(self.conv(g1), self.pool_window, strides=self.pool_stride)\n",
    "        return jnp.multiply(output, g2).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create synapses that model attention operations in modern networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AttentionSynapse(Synapse):\n",
    "    \"\"\"A generalized synapse of quadratic order, whose update rule looks very similar to the Attention operation of Transformers.\n",
    "\n",
    "    We can specify any Lagrangian non-linearity for the hidden neuron layer (which operates with tau=0), but we default to the Softmax Lagrangian.\n",
    "\n",
    "    To replicate similar configuration to the famous BERT-base models and \"Attention is all you need\" paper:\n",
    "\n",
    "    ```\n",
    "    zspace = 64\n",
    "    syn = AttentionSynapse(zspace_dim=zspace, num_heads=12, hidden_lagrangian=LSoftmax(beta=1/jnp.sqrt(zspace)))\n",
    "    ```\n",
    "\n",
    "    Connecting two layers of shapes: ((Nq, Dq), (Nk, Dk)) and layernorm lagrangians\n",
    "    \"\"\"\n",
    "\n",
    "    Wk: jnp.ndarray = tx.Parameter.node()\n",
    "    Wq: jnp.ndarray = tx.Parameter.node()\n",
    "    hidden_lagrangian: Optional[tx.Module]  # An already initialized lagrangian function\n",
    "    qk_norm: Optional[tx.Module]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int = 1,\n",
    "        zspace_dim: int = 64,\n",
    "        stdinit: float = 0.02,\n",
    "        hidden_lagrangian: tx.Module = LSoftmax(),\n",
    "        do_qk_norm: bool = False,\n",
    "    ):\n",
    "        self.zspace_dim = zspace_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.stdinit = stdinit\n",
    "        self.hidden_lagrangian = hidden_lagrangian\n",
    "        self.do_qk_norm = do_qk_norm\n",
    "\n",
    "    def __call__(self, gq, gk):\n",
    "        \"\"\"Align the queries in gq with the keys in gk\"\"\"\n",
    "        if self.initializing():\n",
    "            self.Wq = nn.initializers.normal(self.stdinit)(\n",
    "                tx.next_key(), (gq.shape[-1], self.num_heads, self.zspace_dim)\n",
    "            )\n",
    "            self.Wk = nn.initializers.normal(self.stdinit)(\n",
    "                tx.next_key(), (gk.shape[-1], self.num_heads, self.zspace_dim)\n",
    "            )\n",
    "            if self.do_qk_norm:\n",
    "                self.qk_norm = tx.LayerNorm().init(\n",
    "                    tx.next_key(), jnp.ones(self.zspace_dim)\n",
    "                )\n",
    "\n",
    "        if len(gq.shape) == 1:\n",
    "            gq = gq[..., None, :]\n",
    "        if len(gk.shape) == 1:\n",
    "            gk = gk[..., None, :]\n",
    "        if len(gq.shape) > 2:\n",
    "            gq = rearrange(gq, \"... d -> (...) d\")\n",
    "        if len(gk.shape) > 2:\n",
    "            gk = rearrange(gk, \"... d -> (...) d\")\n",
    "        Q = jnp.einsum(\"qd,dhz->qhz\", gq, self.Wq)\n",
    "        K = jnp.einsum(\"kd,dhz->khz\", gk, self.Wk)\n",
    "        # QK = jnp.einsum(\"qhz,khz->hqk\", Q, K)\n",
    "        if self.do_qk_norm:\n",
    "            Q = self.qk_norm(Q)\n",
    "            K = self.qk_norm(K)\n",
    "        QK = jnp.einsum(\"qhz,khz->hqk\", Q, K)\n",
    "\n",
    "        # print(\"MAX QK: \", QK.max())\n",
    "        # print(\"MIN QK: \", QK.min())\n",
    "        # , QK.min(), gk.max(), gq.max())\n",
    "        attn = self.hidden_lagrangian(QK)  # h,q\n",
    "        return attn.sum()\n",
    "\n",
    "\n",
    "class SelfAttentionSynapse(AttentionSynapse):\n",
    "    \"\"\"A special case of the AttentionSynapse where both inputs are of the same layer\"\"\"\n",
    "\n",
    "    def __call__(self, g):\n",
    "        return super().__call__(g, g)\n",
    "\n",
    "    \n",
    "class BinaryMixerSynapse(Synapse):\n",
    "    \"\"\"A generalized binary synapse of quadratic order. This synapse is very similar to the Attention synapse but uses a single\n",
    "    weight matrix instead of a query and key matrix.\n",
    "\n",
    "    We can specify any Lagrangian non-linearity for the hidden neuron layer (which operates with tau=0), but we default to the Softmax Lagrangian.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    W: jnp.ndarray = tx.Parameter.node()\n",
    "    hidden_lagrangian: Optional[tx.Module]  # An already initialized lagrangian function\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int = 1,\n",
    "        zspace_dim: int = 64,\n",
    "        stdinit: float = 0.02,\n",
    "        hidden_lagrangian: tx.Module = LSoftmax(),\n",
    "    ):\n",
    "        self.num_heads = num_heads\n",
    "        self.zspace_dim=zspace_dim\n",
    "        self.stdinit = stdinit\n",
    "        self.hidden_lagrangian = hidden_lagrangian\n",
    "\n",
    "    def __call__(self, ga, gb):\n",
    "        \"\"\"Align the activations ga with gb\"\"\"\n",
    "        if self.initializing():\n",
    "            self.W =  nn.initializers.normal(self.stdinit)(\n",
    "                tx.next_key(), (self.num_heads, self.zspace_dim, ga.shape[-1], gb.shape[-1])\n",
    "            )\n",
    "\n",
    "        if len(ga.shape) == 1:\n",
    "            ga = ga[...,None,:]\n",
    "        if len(gb.shape) == 1:\n",
    "            gb = gb[...,None,:]\n",
    "        if len(ga.shape) > 2:\n",
    "            ga = rearrange(ga, \"... d -> (...) d\")\n",
    "        if len(gb.shape) > 2:\n",
    "            gb = rearrange(gb, \"... d -> (...) d\")\n",
    "\n",
    "        AB = jnp.einsum(\"hzab,...a,...b->hz\", self.W, ga, gb)\n",
    "        attn = self.hidden_lagrangian(AB)  # h\n",
    "        return attn.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hamux]",
   "language": "python",
   "name": "conda-env-hamux-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
