# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_synapses.ipynb.

# %% auto 0
__all__ = ['Synapse', 'SimpleDenseSynapse', 'DenseSynapse', 'ConvSynapse', 'ConvSynapseWithPool']

# %% ../nbs/02_synapses.ipynb 3
import jax
import jax.numpy as jnp
from typing import *
import treex as tx
from abc import ABC, abstractmethod
from flax import linen as nn
from .lagrangians import *
import functools as ft
from fastcore.meta import delegates
from fastcore.utils import *
from fastcore.basics import *
from string import ascii_letters
from flax.linen.pooling import max_pool, avg_pool
from einops import rearrange

# %% ../nbs/02_synapses.ipynb 4
class Synapse(tx.Module, ABC):
    """The simple interface class for any synapse. Define an alignment function through `__call__` that returns a scalar.

    The energy is simply the negative of this function.
    """

    def energy(self, *gs):
        return -self(*gs)

    @abstractmethod
    def __call__(self, *gs):
        """The alignment function of a synapse"""
        pass

# %% ../nbs/02_synapses.ipynb 6
class SimpleDenseSynapse(Synapse):
    """The simplest of dense synapses that connects two layers (with vectorized activations) together"""
    W: jnp.ndarray = tx.Parameter.node() # treex's preferred way of declaring an attribute as a parameter
    def __call__(self, g1, g2):
        if self.initializing():
            self.W = nn.initializers.normal(0.02)(tx.next_key(), g1.shape + g2.shape)
        return g1 @ self.W @ g2

# %% ../nbs/02_synapses.ipynb 9
class DenseSynapse(Synapse):
    """A dense synapse that aligns the representations of any number of gs.

    The learnable parameter here is a tensor with a dimension for each connected layer.
    The case of 2 layers is a traditional learnable matrix synapse.
    More than 2 layers is a new kind of layer where the learnable parameters are a tensor.

    By default, will flatten all inputs as needed to treat all activations as vectors. Otherwise, treat the last dimension as the operating
    dimension of tensor operations.
    The number of layers we can align with this synapse is capped at the number of ranks that JAX stores (<255),
    but you'll probably run out of memory first..
    """

    W: jnp.ndarray = tx.Parameter.node()
    stdinit: float = 0.02
    flatten_args: bool = True

    def __init__(self, stdinit: float = 0.02, flatten_args=True):
        self.stdinit = stdinit
        self.flatten_args = flatten_args

    def __call__(self, *gs):
        if self.initializing():
            ndims_total = jnp.sum(jnp.array([len(g.shape) for g in gs]))
            assert (
                ndims_total <= 52
            ), f"We are limited to english ASCII letters. We cannot connect more than 52 dimensions. Got {ndims_total} total dimensions."
            if self.flatten_args:
                gshapes = tuple([g_.size for g_ in gs])
            else:
                gshapes = tuple([g_.shape[-1] for g_ in gs])
            self.W = nn.initializers.normal(self.stdinit)(tx.next_key(), gshapes)
        if self.flatten_args:
            gs = [g_.ravel() for g_ in gs]
            abcs = ascii_letters[: len(gs)]
            einsum_arg = ",".join([abcs, ",".join(abcs)]) + "->"
            return jnp.einsum(einsum_arg, self.W, *gs)
        else:
            # Design the einsum to take letter positions corresponding to the
            Wabcs = ""
            gabcs = []
            i = 0
            for g in gs:
                ndims = len(g.shape)
                Wabcs += ascii_letters[(i - 1) + ndims]
                gabcs.append(ascii_letters[i : i + ndims])
                i = i + ndims
            einsum_arg = ",".join([Wabcs, ",".join(gabcs)]) + "->"
            return jnp.einsum(einsum_arg, self.W, *gs)

# %% ../nbs/02_synapses.ipynb 12
class ConvSynapse(Synapse):
    """A convolutional, binary synapse. Can automatically detect the number of output features from the 2 layers it connects"""

    conv: tx.Conv
    # Delegate arguments to conv EXCEPT the features_out, which we calculate from the output layer

    @delegates(tx.Conv)
    def __init__(self, kernel_size: Union[int, Iterable[int]], **kwargs):
        # assert pool_type in ["max", "avg"]
        self.kernel_size = kernel_size
        conv_kwargs = {"use_bias": False}
        conv_kwargs.update(kwargs)
        self.conv_kwargs = conv_kwargs

    def example_output(self, g1, features_out=1):
        """Test the shape output of the convolutional layer. If unspecified, output features are 1"""
        conv = tx.Conv(features_out, self.kernel_size, **self.conv_kwargs).init(
            jax.random.PRNGKey(0), g1
        )
        return conv(g1)

    def __call__(self, g1, g2):
        """The convolutional operation. g2 is assumed to be the "output" of the convolution"""
        if self.initializing():
            features_out = g2.shape[-1]
            self.conv = tx.Conv(
                features_out, self.kernel_size, **self.conv_kwargs
            ).init(tx.next_key(), g1)
        return jnp.multiply(self.conv(g1), g2).sum()

# %% ../nbs/02_synapses.ipynb 14
class ConvSynapseWithPool(Synapse):
    """A convolutional, binary synapse. Can automatically detect the number of output features from the 2 layers it connects"""

    conv: tx.Conv
    # Delegate arguments to conv EXCEPT the features_out, which we calculate from the output layer

    @delegates(tx.Conv)
    def __init__(
        self,
        kernel_size: Union[int, Iterable[int]],
        pool_window=(5, 5),
        pool_stride=(2, 2),
        pool_type="avg",
        **kwargs,
    ):
        # assert pool_type in ["max", "avg"]
        self.kernel_size = kernel_size
        self.conv_kwargs = kwargs
        self.pool_window = pool_window
        self.pool_stride = pool_stride
        self.pool_type = pool_type

        self.pooler = max_pool if self.pool_type == "max" else avg_pool
        # self.conv = None

    def example_output(self, g1, features_out=1):
        """Test the shape output of the convolutional layer. If unspecified, output features are 1"""
        conv = tx.Conv(features_out, self.kernel_size, **self.conv_kwargs).init(
            jax.random.PRNGKey(0), g1
        )
        output = self.pooler(conv(g1), self.pool_window, strides=self.pool_stride)
        return output

    def __call__(self, g1, g2):
        """The convolutional operation. g2 is assumed to be the "output" of the convolution"""
        if self.initializing():
            features_out = g2.shape[-1]
            self.conv = tx.Conv(
                features_out, self.kernel_size, **self.conv_kwargs
            ).init(tx.next_key(), g1)
        output = self.pooler(self.conv(g1), self.pool_window, strides=self.pool_stride)
        return jnp.multiply(output, g2).sum()

