# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_ham.ipynb.

# %% auto 0
__all__ = ['HAM']

# %% ../nbs/03_ham.ipynb 2
import jax
import jax.numpy as jnp
from typing import *
import treex as tx
from .layers import Layer
from .synapses import Synapse
import jax.tree_util as jtu
from .utils import pytree_save, pytree_load, to_pickleable, align_with_state_dict
import pickle
import functools as ft

# %% ../nbs/03_ham.ipynb 5
class HAM(tx.Module):
    layers: List[Layer]
    synapses: List[Synapse]
    connections: List[Tuple[Tuple, int]]

    def __init__(self, layers, synapses, connections):
        self.layers = layers
        self.synapses = synapses
        self.connections = connections

    @property
    def n_layers(self):
        return len(self.layers)

    @property
    def n_synapses(self):
        return len(self.synapses)

    @property
    def n_connections(self):
        return len(self.connections)

    @property
    def layer_taus(self):
        return [layer.tau for layer in self.layers]

    def layer_energy(self, states):
        energies = jnp.stack([self.layers[i].energy(x) for i, x in enumerate(states)])
        return jnp.sum(energies)

    def activations(self, states):
        gs = [self.layers[i].g(states[i]) for i in range(len(states))]
        return gs

    def vactivations(self, states):
        return jax.vmap(self.activations, in_axes=self._statelist_batch_axes())(states)

    def synapse_energy(self, gs):
        """Calculate the synapse energies"""

        def get_energy(lset, k):
            mygs = [gs[i] for i in lset]
            synapse = self.synapses[k]
            return synapse.energy(*mygs)

        energies = jnp.stack([get_energy(lset, k) for lset, k in self.connections])
        return jnp.sum(energies)

    def energy(self, states):
        gs = self.activations(states)
        energy = self.layer_energy(states) + self.synapse_energy(gs)
        return energy

    def venergy(self, states):
        return jax.vmap(self.energy, in_axes=self._statelist_batch_axes())(states)

    def __call__(self, states):
        return self.energy(states)

    def dEdg(self, states):
        """Calculate the gradient of system energy wrt. the activations

        Here, we can take a shortcut: dE_layer/dg = x.
        """
        gs = self.activations(states)
        return jtu.tree_map(
            lambda x, s: x + s, states, jax.grad(self.synapse_energy)(gs)
        )

    def vdEdg(self, states):
        """A vectorized version of dEdg"""
        return jax.vmap(self.dEdg, in_axes=self._statelist_batch_axes())(states)

    def updates(self, states):
        return jtu.tree_map(lambda dg: -dg, self.dEdg(states))

    def vupdates(self, states):
        return jax.vmap(self.updates, in_axes=self._statelist_batch_axes())(states)

    def dEdx(self, states):
        """An alternative method to dEdg for descending the energy function, though it is less computationally efficient and is non-biologically pleasing since it relies on the energy derivative wrt x.

        The energy minima of this method and dEdg remain the same, but the trajectory will be different.
        """
        return jax.grad(self.energy)(states)

    def vdEdx(self, states):
        """A vectorized version of dEdx"""
        return jax.vmap(self.dEdx, in_axes=self._statelist_batch_axes())(states)

    def step(
        self,
        states: List[jnp.ndarray],
        updates: List[jnp.ndarray],
        dt: float = 0.1,
        masks: Optional[List[jnp.ndarray]] = None,
    ):
        """Calculate new states using a simple stepping function.

        A discrete step down the energy using step size `dt` scaled by the `tau` of each layer

        Args:
            states: Current states at time t for each layer
            updates: The direction in which to descend the gradient, corresponding to the structure of `states`
            dt: Step size
            masks: A list of masked boolean arrays, corresponding to the shape of `states`.
                0 means "don't update this neuron in time"
        """
        taus = self.layer_taus
        alphas = [
            dt / tau for tau in taus
        ]  # Scaling factor of the update size of each layer
        if masks is not None:
            xs = jtu.tree_map(
                lambda x, u, m, alpha: x + alpha * u * m, states, updates, masks, alphas
            )
        else:
            xs = jtu.tree_map(
                lambda x, u, alpha: x + alpha * u, states, updates, alphas
            )
        return xs

    def _statelist_batch_axes(self):
        return ([0 for _ in range(self.n_layers)],)

    def init_states(self, bs=None, rng=None):
        if rng is not None:
            keys = jax.random.split(rng, self.n_layers)
            return [layer.init_state(bs, rng=key) for layer, key in zip(self.layers, keys)]
        return [layer.init_state(bs) for layer in self.layers]

    def init_states_and_params(self, param_key, bs=None, state_key=None):
        # params don't need a batch size to initialize
        params = self.init(param_key, self.init_states(), call_method="energy")
        states = self.init_states(bs, rng=state_key)
        return states, params

    def load_state_dict(self, state_dict):
        if not self.initialized:
            _, self = self.init_states_and_params(jax.random.PRNGKey(0), 1)
        self.connections = state_dict["connections"]
        self.layers = align_with_state_dict(self.layers, state_dict["layers"])
        self.synapses = align_with_state_dict(self.synapses, state_dict["synapses"])
        return self

    def save_state_dict(self, fname):
        to_save = jtu.tree_map(to_pickleable, self.to_dict())
        pytree_save(to_save, fname, overwrite=True)

    def load_ckpt(self, ckpt_f):
        with open(ckpt_f, "rb") as fp:
            state_dict = pickle.load(fp)
        return self.load_state_dict(state_dict)

